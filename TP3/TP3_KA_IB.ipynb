{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center> TP3: NN and CNN with ```pytorch``` </center></h1>\n",
    "\n",
    "The deadline for report submission is Tuesday, December 22th 2020.\n",
    "\n",
    "Note: the goal of this TP is to become familiar with 'Pythor' and to understand how to implement Neural Nets with Pyhtor.\n",
    "\n",
    "We first list the basic function in Pythor and consider a very simple example to understand how Grandient Descent can be implemented. Then we illustrate how set the architecture of neural nets and run it on MNIST dataset. Lastly, we provide an implementation of CNN.\n",
    "\n",
    "As a homework, we propose you implement logistic regression as a neural net and to also to add dropout in CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch operates with tensors instead of numpy arrays. Almost everything you can do with numpy arrays can be acomplished with pytorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1642, 0.3461, 0.3179],\n",
      "        [0.8086, 0.2378, 0.6380],\n",
      "        [0.9654, 0.0469, 0.2890]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(3, 3) # random tensor of size 3 by 3\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the result of:\n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      " +\n",
      " tensor([[4., 4., 4.],\n",
      "        [4., 4., 4.],\n",
      "        [4., 4., 4.]]) \n",
      " = \n",
      " tensor([[5., 5., 5.],\n",
      "        [5., 5., 5.],\n",
      "        [5., 5., 5.]])\n"
     ]
    }
   ],
   "source": [
    "# We can operate with pytorch tensors pretty much in the same manner as with numpy arrays\n",
    "x = torch.ones(3,3)\n",
    "y = torch.ones(3,3) * 4\n",
    "z = x + y\n",
    "print(f'This is the result of:\\n {x}\\n +\\n {y} \\n = \\n {z}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From \n",
      " tensor([[5., 5., 5.],\n",
      "        [5., 5., 5.],\n",
      "        [5., 5., 5.]]) we can look at the last column and 2 rows \n",
      " tensor([5., 5.])\n"
     ]
    }
   ],
   "source": [
    "# again we can operate with tensor indexing as if it was a numpy one\n",
    "\n",
    "x = torch.ones(3,3) * 5\n",
    "y = x[-1, :2]\n",
    "print(f'From \\n {x} we can look at the last column and 2 rows \\n {y}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you know, a lot of ML algorithms can be stated as optimization problems.\n",
    "Let us consider a toy example: imagine that our data is $x = (1, \\ldots, 1)^\\top \\in \\mathbb{R}^{5}$ is a vector composed of all ones and a label $y = 1$. We would like to find a weight vector $w \\in \\mathbb{R}^{5}$ such that the loss function $L(w) = (y - x^\\top w)^2$ is minimized.\n",
    "\n",
    "Of course, this is a simple least squares on a single observation $(x, y)$ and we can compute the result analytically. But it is a good example to understand what pytorch has to offer.\n",
    "\n",
    "If we are too lazy to compute the analytic expression, we can run the Gradient Descent, which starts from $w_0 = (0, \\ldots, 0)^\\top$ and proceeds as\n",
    "\n",
    "$$w_k = w_{k - 1} - \\eta \\nabla L(w_{k - 1}).$$\n",
    "\n",
    "So the only thing that we need to know is the gradient of the loss function $L$ evaluated at the point $w_{k - 1}$.\n",
    "Here how it is done in pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.]])\n"
     ]
    }
   ],
   "source": [
    "# Input data\n",
    "y = torch.ones(1, 1)\n",
    "x = torch.ones(1, 5)\n",
    "\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these tensors during the backward pass.\n",
    "w = torch.zeros(5, 1, requires_grad=True) # setting w_0 = (0, ..., 0)^T\n",
    "\n",
    "y_pred = x.mm(w) # inner product of w and x \n",
    "\n",
    "loss = (y - y_pred).pow(2) # squared loss\n",
    "\n",
    "\n",
    "# Use autograd to compute the backward pass. This call will compute the\n",
    "# gradient of loss with respect to all tensors with requires_grad=True.\n",
    "# After this call w.grad will be a tensor holding the gradient\n",
    "# of the loss with respect to w.\n",
    "loss.backward()\n",
    "\n",
    "print(w.grad) # Print the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question.** Assuming that $w_0 = (0, \\ldots, 0)^\\top$ compute on paper $\\nabla L(w_0)$. Do not include the answer to this question into the report. Just make sure you understant what is going on here.\n",
    "\n",
    "Once you made sure that ```w.grad``` indeed stores the value of $\\nabla L(w_0)$. We can implement the Gradient Descent algorithm with only few lines of code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10/150, Current loss: 0.150094673037529\n",
      "Iteration 20/150, Current loss: 0.018248017877340317\n",
      "Iteration 30/150, Current loss: 0.002218528650701046\n",
      "Iteration 40/150, Current loss: 0.00026972233899869025\n",
      "Iteration 50/150, Current loss: 3.279230440966785e-05\n",
      "Iteration 60/150, Current loss: 3.986556748714065e-06\n",
      "Iteration 70/150, Current loss: 4.846697265747935e-07\n",
      "Iteration 80/150, Current loss: 5.8908199207508005e-08\n",
      "Iteration 90/150, Current loss: 7.173785121494802e-09\n",
      "Iteration 100/150, Current loss: 8.74024408403784e-10\n",
      "Iteration 110/150, Current loss: 1.0756195933936397e-10\n",
      "Iteration 120/150, Current loss: 1.3219647598816664e-11\n",
      "Iteration 130/150, Current loss: 1.566746732351021e-12\n",
      "Iteration 140/150, Current loss: 1.7408297026122455e-13\n",
      "Iteration 150/150, Current loss: 1.2789769243681803e-13\n",
      "Final result: tensor([[0.2000],\n",
      "        [0.2000],\n",
      "        [0.2000],\n",
      "        [0.2000],\n",
      "        [0.2000]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Input data\n",
    "y = torch.ones(1, 1)\n",
    "x = torch.ones(1, 5)\n",
    "\n",
    "w = torch.zeros(5, 1, requires_grad=True) # Initialization: w_0 = (0, ..., 0)^T\n",
    "\n",
    "lr = .01 # Learning rate a.k.a. the step size\n",
    "max_iter = 150\n",
    "\n",
    "for k in range(max_iter):\n",
    "    loss = (y - x.mm(w)).pow(2) # forward pass\n",
    "    \n",
    "        \n",
    "    loss.backward() # the backward pass\n",
    "    \n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    with torch.no_grad():\n",
    "        w -= lr * w.grad # gradient step\n",
    "        w.grad.zero_() # after performing operation with gradient we need to erase it\n",
    "    \n",
    "    if k % 10 == 9:\n",
    "        print(f'Iteration {k + 1}/{max_iter}, Current loss: {loss.item()}')\n",
    "        \n",
    "print(f'Final result: {w}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Solve the problem $\\min_{w \\in \\mathbb{R}^5}\\, (1 - x^\\top w)^2$ with $x = (1, \\ldots, 1)^\\top \\in \\mathbb{R}^5$ analytically and compare to the result of the Gradient Descent.\n",
    "\n",
    "**Question:** Recalling the theory of numerical optimization, what is the learning rate ```lr``` that we need to set sto ensure the fastest convergence?\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "The learning rate is a hyperparameter that controls how much to change the model in response to the estimated error each time the model weights are updated. Choosing the learning rate is challenging as a value too small may result in a long training process that could get stuck, whereas a value too large may result in learning a sub-optimal set of weights too fast or an unstable training process.\n",
    "The learning rate may be the most important hyperparameter when configuring our neural network.\n",
    "\n",
    "$$w_k = w_{k - 1} - \\eta \\nabla L(w_{k - 1}).$$\n",
    "\n",
    "The learning rate ```lr``` is η in the previous Gradient Descent's formula, if we set :\n",
    "\n",
    "**η to be a large value** → learn too much (rapid learning) : Unable to converge to a good local minima (unable to effectively gradually decrease our loss, overshoot the local lowest value)\n",
    "\n",
    "**η to be a small value** → learn too little (slow learning) : May take too long or unable to convert to a good local minima\n",
    "\n",
    "Reference : https://machinelearningmastery.com/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks/\n",
    "https://www.deeplearningwizard.com/deep_learning/boosting_models_pytorch/lr_scheduling/\n",
    "\n",
    "Learning rate determines how fast the algorithm learns. Too small and the algorithm learns too slowly, too large and the algorithm learns too fast resulting in instabilities.\n",
    "\n",
    "**Question:** Explain the connection of ```loss.backward()``` and the backpropagation for feedforward neural nets.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "Backpropagation is a short form for \"backward propagation of errors.\" \n",
    "Technically, the backpropagation algorithm is a method for training the weights in a multilayer feed-forward neural network. As such, it requires a network structure to be defined of one or more layers where one layer is fully connected to the next layer. A standard network structure is one input layer, one hidden layer, and one output layer.\n",
    "The backpropagation computes the gradient of the loss function with respect to the weights of the network. This helps to update weights to minimize loss. There are many update rules for updating the weights mainly Gradient descent, Stochastic gradient descent, RMSProp, Adam.\n",
    "\n",
    "```loss.backward()``` computes gradient of loss (dloss/dw) w.r.t all the parameters (w) in loss that have requires_grad = True and store them in parameter.grad (w.grad) attribute for every parameter (w).\n",
    "\n",
    "Reference : https://medium.com/analytics-vidhya/backpropagation-algorithm-using-pytorch-ee1287888aca\n",
    "\n",
    "Here, the output values are compared with the correct answer to compute the value of some predefined error-function. By various techniques, the error is then fed back through the network. Using this information, the algorithm adjusts the weights of each connection in order to reduce the value of the error function by some small amount. After repeating this process for a sufficiently large number of training cycles, the network will usually converge to some state where the error of the calculations is small. In this case, one would say that the network has learned a certain target function. To adjust weights properly, one applies a general method for non-linear optimization that is called gradient descent. For this, the network calculates the derivative of the error function with respect to the network weights, and changes the weights such that the error decreases (thus going downhill on the surface of the error function). For this reason, back-propagation can only be applied on networks with differentiable activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi layer perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will build our neural net. Recall that MNIST is composed of images of size $28 \\times 28$, hence the dimenison of the input is $784$. We have $10$ classes, so the dimension of the output is $10$.\n",
    "\n",
    "In between we will insert $2$ hidden layers and use ReLU as our non-linearity (activation function).\n",
    "The first hidden layer is composed of $128$ neurons and the second one of $64$ neurons.\n",
    "\n",
    "We will not use GPU nor we will consider complicated neural nets in this TP. The goal is to introduce you to the basics without going into too complicated architechtures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleFeedForward(nn.Module):\n",
    "    def __init__(self, input_size=784, hidden_sizes=[128, 64],\n",
    "                 output_size=10):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_sizes[0]), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_sizes[1], output_size)\n",
    "        )\n",
    "             \n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, input_size)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we defined our neural net we need to train it.\n",
    "The training is going to be performed via Stochastic Gradient Descent evaluated on a mini batch of the data.\n",
    "That is, on the foward stage we will use not a single data point but several ones. In this case we set the size of mini batch equal to $32$.\n",
    "\n",
    "Actually, size of the mini batch, learning rate sizes of hidden layers are all considered as hyperparameters that can be finely tuned (some people even tune random seed, which is absolutely ridiculous). We will not talk about the hypeparameter tuning in this TP, to learn more have a look at https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html .\n",
    "\n",
    "\n",
    "**Important:** We do not require you to perform complicated hyperparameter tuning. This part is beyond the course. However, it is important that you can clearly write an architechture of a nerual net that you consider.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training consists of gradient steps over mini batch of data\n",
    "def train(model, trainloader, loss, optimizer, epoch, num_epochs):\n",
    "    # We enter train mode. This is useless for the linear model\n",
    "    # but is important for layers such as dropout, batchnorm, ...\n",
    "    model.train()\n",
    "    \n",
    "    loop = tqdm(trainloader)\n",
    "    loop.set_description(f'Training Epoch [{epoch + 1}/{num_epochs}]')\n",
    "    \n",
    "    # We iterate over the mini batches of our data\n",
    "    for inputs, targets in loop:\n",
    "    \n",
    "        # Erase any previously stored gradient\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        \n",
    "        outputs = net(inputs) # Forwards stage (prediction with current weights)\n",
    "        loss = criterion(outputs, targets) # loss evaluation\n",
    "        \n",
    "        loss.backward() # Back propagation (evaluate gradients) \n",
    "        \n",
    "        \n",
    "        # Making gradient step on the batch (this function takes care of the gradient step for us)\n",
    "        optimizer.step() \n",
    "        \n",
    "def validation(model, valloader, loss):\n",
    "    # Do not compute gradient, since we do not need it for validation step\n",
    "    with torch.no_grad():\n",
    "        # We enter evaluation mode.\n",
    "        model.eval()\n",
    "        \n",
    "        total = 0 # keep track of currently used samples\n",
    "        running_loss = 0.0 # accumulated loss without averagind\n",
    "        accuracy = 0.0 # accumulated accuracy without averagind (number of correct predictions)\n",
    "        \n",
    "        loop = tqdm(valloader) # This is for the progress bar\n",
    "        loop.set_description('Validation in progress')\n",
    "        \n",
    "        \n",
    "        # We again iterate over the batches of validation data. batch_size does not play any role here\n",
    "        for inputs, targets in loop:\n",
    "            # Run samples through our net\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Total number of used samples\n",
    "            total += inputs.shape[0]\n",
    "\n",
    "            # Multiply loss by the batch size to erase averagind on the batch\n",
    "            running_loss += inputs.shape[0] * loss(outputs, targets).item()\n",
    "            \n",
    "            # how many correct predictions\n",
    "            accuracy += (outputs.argmax(dim=1) == targets).sum().item()\n",
    "            \n",
    "            # set nice progress meassage\n",
    "            loop.set_postfix(val_loss=(running_loss / total), val_acc=(accuracy / total))\n",
    "        return running_loss / total, accuracy / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use again the MNIST dataset. This time we will use the official train/test split!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We download the oficial MNIST train set\n",
    "all_train = datasets.MNIST('data/',\n",
    "                           download=True,\n",
    "                           train=True,\n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "# We split the whole train set in two parts:\n",
    "# the one that we actually use for training\n",
    "# and the one that we use for validation\n",
    "batch_size = 32 # size of the mini batch\n",
    "num_train = int(0.8 * len(all_train))\n",
    "\n",
    "trainset, valset = torch.utils.data.random_split(all_train, [num_train, len(all_train) - num_train])\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of the inputs are torch.Size([32, 1, 28, 28])\n",
      "The number on the image is: 2\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we can iterate over trainloader in the following way\n",
    "for inputs, targets in trainloader:\n",
    "    print(f'Dimensions of the inputs are {inputs.shape}')\n",
    "    plt.imshow(inputs[0][0], cmap='gray', interpolation='none')\n",
    "    print(f'The number on the image is: {targets[0]}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of ```inputs``` is $(32, 1, 28, 28)$. The first dimension indicates the size of the mini batch and is controlled by parameter ```batch_size```, the last two parameters are the 2D dimensions of the image and are equal to $28 \\times 28$ in case of the MNIST data. The lonely $1$, staying in the second dimension essentialy reflects the fact that the images are black and white. For instance, if MNIST were colored (there are variants of colored MNIST actually), then we would need $3$ (in case of RGB) colors to represent an image, thus $1$ would be replaed by $3$. \n",
    "\n",
    "**Question:** Run the above block several times. Is it plotting the same number all the time? If not, why?\n",
    "\n",
    "**Answer:** No, because ```torch.utils.data.random_split``` randomly split the dataset all_train into non-overlapping new datasets (trainset and valset).\n",
    "We can optionally fix the generator for reproducible results, e.g.:\n",
    "```trainset, valset = torch.utils.data.random_split(all_train, [num_train, len(all_train) - num_train], generator=torch.Generator().manual_seed(42))```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Net + training parameters\n",
    "num_epochs = 2 # how many passes over the whole train data\n",
    "input_size = 784 # flattened size of the image\n",
    "hidden_sizes = [128, 64] # sizes of hidden layers\n",
    "output_size = 10 # how many labels we have\n",
    "lr = 0.001 # learning rate\n",
    "momentum = 0.9 # momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing our model/loss/optimizer\n",
    "net = SimpleFeedForward(input_size, hidden_sizes, output_size) # Our neural net\n",
    "criterion = nn.CrossEntropyLoss() # Loss function to be optimized\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr, momentum=momentum) # Optimization algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26d88d7dc5e64a93a36082329a9d60fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "726b38ef3f554748b1898fc7818ca31a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=375.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d5349756422451588bee7d5c4eb20ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "122b5b18370e4c6fad679437d90f2a7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=375.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# num_epochs indicates the number of passes over the data\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # makes one pass over the train data and updates weights\n",
    "    train(net, trainloader, criterion, optimizer, epoch, num_epochs)\n",
    "\n",
    "    # makes one pass over validation data and provides validation statistics\n",
    "    val_loss, val_acc = validation(net, valloader, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4388c6d8afe8445b9d275dd07a3ea513",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=313.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy: 0.9022 | Test loss: 0.3439668979883194\n"
     ]
    }
   ],
   "source": [
    "# Let us evaluate our net on the test set that we have never seen!\n",
    "testset = datasets.MNIST('data/',\n",
    "                         download=True,\n",
    "                         train=False,\n",
    "                         transform=transforms.ToTensor())\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loss, test_acc = validation(net, testloader, criterion)\n",
    "print(f'Test accuracy: {test_acc} | Test loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Logistic regression via pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using above code as an example implement multinomial logistic regression and train it on the same data.\n",
    "For your report include:\n",
    "1. Mathematical description of logistic regression\n",
    "2. Mathematical description of optimization algorithm that you use\n",
    "3. High level idea of how to implement logisitic regression with pytorch\n",
    "4. Report classification accuracy on test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer :**\n",
    "Logistic Regression can be thought of as a simple, fully-connected neural network with one hidden layer.\n",
    "For the multinomial logistic regression the function is the Softmax Function. \n",
    "<mark> @Kim j'ai fait le code ci-dessous mais il faudra qu'on ajoute la partie description mathématique qui est expliqué sur la page référencée :</mark>\n",
    "References : https://aaronkub.com/2020/02/12/logistic-regression-with-pytorch.html\n",
    "Exemples de code que lequel je me suis inspirée mais en restant proche du code du prof car ces codes diffèrent énormément : https://github.com/liaison/Multinomial-Logistic-Regression/blob/master/MNL.py et : https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/01-basics/logistic_regression/main.py\n",
    "\n",
    "<mark> @Kim par contre le test accuracy n'est pas bon il doit y avoir une petite erreur dans mon code mais c'est un peu tard et du coup je n'arrive pas à trouver ce soir :( ... </mark>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add multinomial logistic regression\n",
    "class MultinomialLogisticRegression(nn.Module):\n",
    "    def __init__(self, input_size=784, output_size=10):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_size, output_size), \n",
    "            nn.Softmax()\n",
    "        )\n",
    "             \n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, input_size)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing our model/loss/optimizer\n",
    "MLR = MultinomialLogisticRegression(input_size, output_size) \n",
    "criterion = nn.CrossEntropyLoss() # Loss function to be optimized\n",
    "#optimizer = optim.SGD(MLR.parameters(), lr=lr, momentum=momentum) # Optimization algorithm\n",
    "optimizer = optim.SGD(MLR.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6ab3b06b4b3410dbd6a363280d15d83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22debeac1ecc4235a6b77dc300c80130",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=375.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97db85d3a57c44d0ae6aa9409f3834f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e6f83d7484343d9a3023dd763d98c98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=375.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# num_epochs indicates the number of passes over the data\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # makes one pass over the train data and updates weights\n",
    "    train(MLR, trainloader, criterion, optimizer, epoch, num_epochs)\n",
    "\n",
    "    # makes one pass over validation data and provides validation statistics\n",
    "    val_loss, val_acc = validation(MLR, valloader, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e87adc46f6474d079387cd1a171921ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=313.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy: 0.0738 | Test loss: 2.3076221118927003\n"
     ]
    }
   ],
   "source": [
    "# Let us evaluate our net on the test set that we have never seen!\n",
    "testset = datasets.MNIST('data/',\n",
    "                         download=True,\n",
    "                         train=False,\n",
    "                         transform=transforms.ToTensor())\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loss, test_acc = validation(MLR, testloader, criterion)\n",
    "print(f'Test accuracy: {test_acc} | Test loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elements of CNN: ```nn.Conv2d``` and ```MaxPool2d```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read this before starting: https://ttic.uchicago.edu/~shubhendu/Pages/Files/Lecture7_flat.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding the convolutional layer in pytorch**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we instanciate ```nn.Conv2d(1, 1, kernel_size=2, stride=[1, 1], padding=0)``` it has a parameter ```weight``` which precisely describes the kernel used for our convolution. In the beginning it is initialized randomly, and our goal is to eventually learn its weights (as usual via backpropagation!).\n",
    "Before building our first CNN let us have a look at the kernel and what it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 input channel (first 1 in nn.Conv2d)\n",
    "# 1 output channel (second 1 in nn.Conv2d)\n",
    "# 2x2 kernel (kernel_size=2)\n",
    "# the kernel slides by one step in (x, y) direction (stride=[1, 1])\n",
    "# we do not augment the picture with white borders (padding=0)\n",
    "conv = nn.Conv2d(1, 1, kernel_size=2, stride=[1, 1], padding=0) \n",
    "# Get kernel value.\n",
    "weight = conv.weight.data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualization.** We will plot the initial image, the kernel, and the resulting image. In order to understand what is going on, the resulting image will be computed in two ways. First of all it will be computed by using ```conv1(image)```. Secondly, we will manually apply the sliding kernel to each $2\\times 2$ window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'By hand')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# take one image\n",
    "image, _ = next(iter(trainloader))\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 4)\n",
    "fig.tight_layout()\n",
    "fig.suptitle('Convolution')\n",
    "\n",
    "# plot the image\n",
    "axs[0].imshow(image[0][0], cmap='gray', interpolation='none')\n",
    "axs[0].set_title('Original image')\n",
    "\n",
    "# plot the kernel\n",
    "axs[1].imshow(weight[0][0], cmap='gray', interpolation='none')\n",
    "axs[1].set_title('2x2 kernel')\n",
    "\n",
    "# plot resulting image\n",
    "axs[2].imshow(conv(image)[0][0].detach().numpy(), cmap='gray', interpolation='none')\n",
    "axs[2].set_title('Resulting image')\n",
    "\n",
    "# Making the same by hands\n",
    "# IMPORTANT: we strongly suggest to understand the below code\n",
    "np_image = image[0][0].data.numpy() # get numpy image\n",
    "image_convolved = np.zeros((27, 27)) # here we store our result\n",
    "for i in range(27):\n",
    "    for j in range(27):\n",
    "        image_convolved[i, j] = np.sum(np_image[i:i+2, j:j+2] * weight) # apply the kernel for each 2x2 window\n",
    "        \n",
    "axs[3].imshow(image_convolved, cmap='gray', interpolation='none')\n",
    "axs[3].set_title('By hand')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem.** Provide 'by hand' implementation of the following kernel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1 input channel (first 1 in nn.Conv2d)\n",
    "# 1 output channel (second 1 in nn.Conv2d)\n",
    "# 4x4 kernel (kernel_size=4)\n",
    "# the kernel slides by 3 step in (x, y) direction (stride=[4, 4])\n",
    "# we do not augment the picture with white borders (padding=0)\n",
    "conv = nn.Conv2d(1, 1, kernel_size=4, stride=[4, 4], padding=0) \n",
    "# Get kernel value.\n",
    "weight = conv.weight.data.numpy()\n",
    "\n",
    "# take one image\n",
    "image, _ = next(iter(trainloader))\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 4)\n",
    "fig.tight_layout()\n",
    "fig.suptitle('Convolution')\n",
    "\n",
    "# plot the image\n",
    "axs[0].imshow(image[0][0], cmap='gray', interpolation='none')\n",
    "axs[0].set_title('Original image')\n",
    "\n",
    "# plot the kernel\n",
    "axs[1].imshow(weight[0][0], cmap='gray', interpolation='none')\n",
    "axs[1].set_title('4x4 kernel')\n",
    "\n",
    "# plot resulting image\n",
    "axs[2].imshow(conv(image)[0][0].detach().numpy(), cmap='gray', interpolation='none')\n",
    "axs[2].set_title('Resulting image')\n",
    "\n",
    "# Making the same by hands\n",
    "# PROBLEM: FILL IN THIS PART. \n",
    "np_image = image[0][0].data.numpy() # get numpy image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding the pooling layer in pytorch**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max pooling is what often used in practice, it amounts to picking only the largest value of a pixel in a given window. In pytorch it is done via ```MaxPool2d(kernel_size=k, stride=s)```, which has two parameters: kernel size and the stride. Note that there are no weights to learn here, so this layer is simply fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'By hand')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# kernel_size -- size of the max pool window\n",
    "pool = nn.MaxPool2d(kernel_size=4, stride=[4,4])\n",
    "\n",
    "fig, axs = plt.subplots(1, 3)\n",
    "fig.tight_layout()\n",
    "fig.suptitle('Pooling')\n",
    "\n",
    "# plot the image\n",
    "axs[0].imshow(image[0][0], cmap='gray', interpolation='none')\n",
    "axs[0].set_title('Original image')\n",
    "\n",
    "\n",
    "# plot resulting image\n",
    "axs[1].imshow(pool(image)[0][0].detach().numpy(), cmap='gray', interpolation='none')\n",
    "axs[1].set_title('Resulting image')\n",
    "\n",
    "# Making the same by hands\n",
    "# IMPORTANT: we strongly suggest to understand the below code\n",
    "np_image = image[0][0].data.numpy() # get numpy image\n",
    "image_pooled = np.zeros((7, 7)) # here we store our result\n",
    "for i in range(0, 27, 4):\n",
    "    for j in range(0, 27, 4):\n",
    "        image_pooled[int(i / 4), int(j / 4)] = np.max(np_image[i:i+4, j:j+4]) # max pooling\n",
    "        \n",
    "axs[2].imshow(image_pooled, cmap='gray', interpolation='none')\n",
    "axs[2].set_title('By hand')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bulding a simple ConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=5, stride=[1, 1], padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(14 * 14 * 8, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 10),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the first layer is ```nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2)```, the parameters here are chosen in such a way that the size of each output channel remains as $28 \\times 28$. Indeed, setting ```padding = 2``` we augmented our initial image to $32 \\times 32$, then we slide a kernel of size $5 \\times 5$ by $1$ in both $(x, y)$ directions which result in a $28 \\times 28$ output image (and $8$ channels).\n",
    "\n",
    "In general the formula for square images and squared kernels is\n",
    "$$\n",
    "    S_{out} = \\frac{S_{in} - S_{kernel} + 2S_{padding}}{S_{stride}} + 1\n",
    "$$\n",
    "\n",
    "In our case it is\n",
    "\n",
    "$$\n",
    "    S_{out} = \\frac{28 - 5 + 4}{1} + 1 = 28\n",
    "$$\n",
    "\n",
    "Then the output of ```nn.Conv2d(1, 8, kernel_size=5, stride=1, padding=2)``` goes into ```nn.ReLU()``` our favorite non-linearity and eventually into the pooling layer ```nn.MaxPool2d(kernel_size=2, stride=2)```.\n",
    "The ```nn.ReLU()``` doe not affect the size, hence ```nn.MaxPool2d(kernel_size=2, stride=2)``` receives $8$ channels of $28 \\times 28$ images as computed above.\n",
    "\n",
    "```nn.MaxPool2d(kernel_size=2, stride=2)``` will be applied to each single channel, with ```kernel_size=2, stride=2``` meaning that the output will still have $8$ channels but the images will be halfed in both $(x, y)$ directions. Hence the output of ```nn.MaxPool2d(kernel_size=2, stride=2)``` has $8$ channels with $14 \\times 14$ images.\n",
    "\n",
    "After all this, we will flatten our features and put the into simple ```nn.Linear(14 * 14 * 8, 500)```, where the input size is precisely the output size of ```nn.MaxPool2d(kernel_size=2, stride=2)```, and $500$ stands for the output size of this linear layer.\n",
    "Finally, we apply our favorite nonlinearity to ```nn.Linear(14 * 14 * 8, 500)``` followed by fully connected linear layer ```nn.Linear(500, 10)``` to match the dimension of $10$ classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28330a962d1f4b25b0e32372c77683ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4618276f80244818bb4e810d6eb31c49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=375.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4e0f89207034f638de741539e2bde1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a59d43c30e65419c905cb090ec2bf0aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=375.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "net = ConvNet()\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # makes one pass over the train data and updates weights\n",
    "    train(net, trainloader, criterion, optimizer, epoch, num_epochs)\n",
    "\n",
    "    # makes one pass over validation data and provides validation statistics\n",
    "    val_loss, val_acc = validation(net, valloader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "805720f0c4534e40a12677e18536b62c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=313.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy: 0.9862 | Test loss: 0.04667040033973754\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = validation(net, testloader, criterion)\n",
    "print(f'Test accuracy: {test_acc} | Test loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see the result here is much better, than in the simple multilayer perceptron. But note, we have actualy trained muuuuuch more parameters here and, at least on my computer, it takes considerably more time.\n",
    "\n",
    "Here you can see the summary of current state of the art results on MNIST: https://www.kaggle.com/c/digit-recognizer/discussion/61480\n",
    "\n",
    "As you see our score barely beats a carefully built random forest or **kNN**! To get extra $0.01$ requires much more fine tuning, which is of course is not the goal here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the code for ConvNet and insert Dropout layer (whereever you want).\n",
    "\n",
    "Include in your report:\n",
    "1. High level description of the dropout\n",
    "2. High level description of your architechture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the succesful completion of this TP, we expect you to be able to understand the architectures of NN, CNN.\n",
    "For instance, have a look at the famous AlexNet https://github.com/pytorch/vision/blob/master/torchvision/models/alexnet.py and see if you can understand its architechture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
