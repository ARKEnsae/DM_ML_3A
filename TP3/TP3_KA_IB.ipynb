{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP3: NN and CNN with `Pytorch` \n",
    "\n",
    "The deadline for report submission is Tuesday, December 22th 2020.\n",
    "\n",
    "Note: the goal of this TP is to become familiar with 'Pythor' and to understand how to implement Neural Nets with Pyhtor.\n",
    "\n",
    "We first list the basic function in Pythor and consider a very simple example to understand how Grandient Descent can be implemented. Then we illustrate how set the architecture of neural nets and run it on MNIST dataset. Lastly, we provide an implementation of CNN.\n",
    "\n",
    "As a homework, we propose you implement logistic regression as a neural net and to also to add dropout in CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import random\n",
    "random.seed(1) #to fix random and have the same results for both of us "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch operates with tensors instead of numpy arrays. Almost everything you can do with numpy arrays can be acomplished with pytorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1642, 0.3461, 0.3179],\n",
      "        [0.8086, 0.2378, 0.6380],\n",
      "        [0.9654, 0.0469, 0.2890]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(3, 3) # random tensor of size 3 by 3\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the result of:\n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      " +\n",
      " tensor([[4., 4., 4.],\n",
      "        [4., 4., 4.],\n",
      "        [4., 4., 4.]]) \n",
      " = \n",
      " tensor([[5., 5., 5.],\n",
      "        [5., 5., 5.],\n",
      "        [5., 5., 5.]])\n"
     ]
    }
   ],
   "source": [
    "# We can operate with pytorch tensors pretty much in the same manner as with numpy arrays\n",
    "x = torch.ones(3,3)\n",
    "y = torch.ones(3,3) * 4\n",
    "z = x + y\n",
    "print(f'This is the result of:\\n {x}\\n +\\n {y} \\n = \\n {z}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From \n",
      " tensor([[5., 5., 5.],\n",
      "        [5., 5., 5.],\n",
      "        [5., 5., 5.]]) we can look at the last column and 2 rows \n",
      " tensor([5., 5.])\n"
     ]
    }
   ],
   "source": [
    "# again we can operate with tensor indexing as if it was a numpy one\n",
    "\n",
    "x = torch.ones(3,3) * 5\n",
    "y = x[-1, :2]\n",
    "print(f'From \\n {x} we can look at the last column and 2 rows \\n {y}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you know, a lot of ML algorithms can be stated as optimization problems.\n",
    "Let us consider a toy example: imagine that our data is $x = (1, \\ldots, 1)^\\top \\in \\mathbb{R}^{5}$ is a vector composed of all ones and a label $y = 1$. We would like to find a weight vector $w \\in \\mathbb{R}^{5}$ such that the loss function $L(w) = (y - x^\\top w)^2$ is minimized.\n",
    "\n",
    "Of course, this is a simple least squares on a single observation $(x, y)$ and we can compute the result analytically. But it is a good example to understand what pytorch has to offer.\n",
    "\n",
    "If we are too lazy to compute the analytic expression, we can run the Gradient Descent, which starts from $w_0 = (0, \\ldots, 0)^\\top$ and proceeds as\n",
    "\n",
    "$$w_k = w_{k - 1} - \\eta \\nabla L(w_{k - 1}).$$\n",
    "\n",
    "So the only thing that we need to know is the gradient of the loss function $L$ evaluated at the point $w_{k - 1}$.\n",
    "Here how it is done in pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y: tensor([[1.]])\n",
      "x: tensor([[1., 1., 1., 1., 1.]])\n",
      "w: tensor([[0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.],\n",
      "        [0.]], requires_grad=True)\n",
      "y_pred: tensor([[0.]], grad_fn=<MmBackward>)\n",
      "loss: tensor([[1.]], grad_fn=<PowBackward0>)\n",
      "w.grad: tensor([[-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.],\n",
      "        [-2.]])\n"
     ]
    }
   ],
   "source": [
    "# Input data\n",
    "y = torch.ones(1, 1) \n",
    "print(\"y:\",y)\n",
    "x = torch.ones(1, 5)\n",
    "print(\"x:\",x)\n",
    "\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these tensors during the backward pass.\n",
    "w = torch.zeros(5, 1, requires_grad=True) # setting w_0 = (0, ..., 0)^T\n",
    "print(\"w:\",w)\n",
    "\n",
    "y_pred = x.mm(w) # inner product of w and x \n",
    "print(\"y_pred:\",y_pred)\n",
    "\n",
    "loss = (y - y_pred).pow(2) # squared loss\n",
    "print(\"loss:\",loss)\n",
    "\n",
    "# Use autograd to compute the backward pass. This call will compute the\n",
    "# gradient of loss with respect to all tensors with requires_grad=True.\n",
    "# After this call w.grad will be a tensor holding the gradient\n",
    "# of the loss with respect to w.\n",
    "loss.backward()\n",
    "\n",
    "print(\"w.grad:\",w.grad) # Print the gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question.** Assuming that $w_0 = (0, \\ldots, 0)^\\top$ compute on paper $\\nabla L(w_0)$. Do not include the answer to this question into the report. Just make sure you understand what is going on here.\n",
    "\n",
    "Once you made sure that ```w.grad``` indeed stores the value of $\\nabla L(w_0)$. We can implement the Gradient Descent algorithm with only few lines of code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 10/150, Current loss: 0.150094673037529\n",
      "Iteration 20/150, Current loss: 0.018248017877340317\n",
      "Iteration 30/150, Current loss: 0.002218528650701046\n",
      "Iteration 40/150, Current loss: 0.00026972233899869025\n",
      "Iteration 50/150, Current loss: 3.279230440966785e-05\n",
      "Iteration 60/150, Current loss: 3.986556748714065e-06\n",
      "Iteration 70/150, Current loss: 4.846697265747935e-07\n",
      "Iteration 80/150, Current loss: 5.8908199207508005e-08\n",
      "Iteration 90/150, Current loss: 7.173785121494802e-09\n",
      "Iteration 100/150, Current loss: 8.74024408403784e-10\n",
      "Iteration 110/150, Current loss: 1.0756195933936397e-10\n",
      "Iteration 120/150, Current loss: 1.3219647598816664e-11\n",
      "Iteration 130/150, Current loss: 1.566746732351021e-12\n",
      "Iteration 140/150, Current loss: 1.7408297026122455e-13\n",
      "Iteration 150/150, Current loss: 1.2789769243681803e-13\n",
      "Final result: tensor([[0.2000],\n",
      "        [0.2000],\n",
      "        [0.2000],\n",
      "        [0.2000],\n",
      "        [0.2000]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Input data\n",
    "y = torch.ones(1, 1)\n",
    "x = torch.ones(1, 5)\n",
    "\n",
    "w = torch.zeros(5, 1, requires_grad=True) # Initialization: w_0 = (0, ..., 0)^T\n",
    "\n",
    "lr = .01 # Learning rate a.k.a. the step size\n",
    "max_iter = 150\n",
    "\n",
    "for k in range(max_iter):\n",
    "    loss = (y - x.mm(w)).pow(2) # forward pass\n",
    "    \n",
    "        \n",
    "    loss.backward() # the backward pass\n",
    "    \n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    with torch.no_grad():\n",
    "        w -= lr * w.grad # gradient step\n",
    "        w.grad.zero_() # after performing operation with gradient we need to erase it\n",
    "    \n",
    "    if k % 10 == 9:\n",
    "        print(f'Iteration {k + 1}/{max_iter}, Current loss: {loss.item()}')\n",
    "        \n",
    "print(f'Final result: {w}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Solve the problem $\\min_{w \\in \\mathbb{R}^5}\\, (1 - x^\\top w)^2$ with $x = (1, \\ldots, 1)^\\top \\in \\mathbb{R}^5$ analytically and compare to the result of the Gradient Descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "If $x = (1, \\ldots, 1)^\\top \\in \\mathbb{R}^5$ and $w = (w_1, \\ldots, w_5)^\\top \\in \\mathbb{R}^5$\n",
    "\n",
    "Let be $f(w) = f(w_1, \\dots w_5) = (1 - x^\\top w)^2 = (1 - w_1 - w_2 - w_3 - w_4 - w_5)^2$\n",
    "\n",
    "As $f$ is a positive function, we clearly see that the minimum of $f$ is 0 and is reached for all the points w such as $\\sum_{k=1}^5 w_k = 1$ \n",
    "\n",
    "In particular the point $w=(0.2, 0.2, 0.2, 0.2, 0.2)$ which is the result of the previous Gradient Descent is one of the minima of the function but the result of the gradient descent would have been different if we had chosen another $w_0 \\ne (0, 0, 0, 0, 0)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Question:** Recalling the theory of numerical optimization, what is the learning rate ```lr``` that we need to set to ensure the fastest convergence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Answer:**\n",
    "\n",
    "The learning rate $\\eta$ is a hyperparameter that determines how fast the algorithm learns. It controls how much to change the model in response to the estimated error each time the model weights are updated following this formula : $w_k = w_{k - 1} - \\eta \\nabla L(w_{k - 1})$. The learning rate is one of the most important hyperparameter when configuring our neural network. Choosing it is challenging because : \n",
    "\n",
    "- **if η is too large**, the model learns too much (rapid learning) and may result in learning a sub-optimal set of weights or an unstable training process  (unable to effectively gradually decrease our loss). The gradient descent can overshoot the local lowest value. It may fail to converge to a good local minimum or may even diverge.\n",
    "\n",
    "- **if η is too small**, the model learns too little (slow learning) and it may take too long to converge or would even get stuck and unable to converge to a good local minima\n",
    "\n",
    "**=> To conclude, we must choose a learning rate which is larger enough to converge fast enough but not to large to prevent from rapid learning (being unable to converge to a good minimum). To choose it, we have to test different hyperparameters and define which is the best compromise (\"hyperparameter tuning\").**\n",
    "\n",
    "*Source : [here](https://machinelearningmastery.com/understand-the-dynamics-of-learning-rate-on-deep-learning-neural-networks/) and [here](https://www.deeplearningwizard.com/deep_learning/boosting_models_pytorch/lr_scheduling/)*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Question:** Explain the connection of ```loss.backward()``` and the backpropagation for feedforward neural nets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Answer:**\n",
    "\n",
    "Backpropagation is a short form for \"backward propagation of errors\".   Technically, the backpropagation algorithm is a method for training the weights in a multilayer feed-forward neural network. As such, it requires a network structure to be defined of one or more layers where one layer is fully connected to the next layer. A standard network structure is one input layer, one hidden layer, and one output layer.\n",
    "The backpropagation computes the gradient of the loss function with respect to the weights of the network. This helps to update weights to minimize loss. There are many update rules for updating the weights : mainly Gradient descent, Stochastic gradient descent, RMSProp, Adam.\n",
    "\n",
    "```loss.backward()``` computes gradient of loss (dloss/dw) w.r.t all the parameters w in loss for which `requires_grad = True`. It stores them in the parameter.grad (`w.grad`) attribute for every parameter w.\n",
    "\n",
    "<mark> @Isabelle : je ne suis pas sûre de comprendre le \"w.r.t\" que tu as mis dans la phrase ci-dessus. Je remplacerais intuitivement par \"for\" est-ce bien cela ? </mark>\n",
    "\n",
    "Here, the output values are compared with the correct answer to compute the value of some predefined error-function. By various techniques, the error is then fed back through the network. Using this information, the algorithm adjusts the weights of each connection in order to reduce the value of the error function by some small amount. After repeating this process for a sufficiently large number of training cycles, the network will usually converge to some state where the error of the calculations is small. In this case, one would say that the network has learned a certain target function. To adjust weights properly, one applies a general method for non-linear optimization that is called gradient descent. For this, the network calculates the derivative of the error function with respect to the network weights, and changes the weights such that the error decreases (thus going downhill on the surface of the error function). For this reason, back-propagation can only be applied on networks with differentiable activation functions.\n",
    "\n",
    "\n",
    "<mark> @Isabelle : Je serai preneuse que tu m'expliques ce que tu as compris dans cette partie oralement car je ne suis pas sûre d'avoir réussi à avoir une vision globale de ce que tu expliques dans le dernier paragraphe et je pense qu'une explication de ta part sera pour moi la meilleure façon de comprendre !! </mark>\n",
    "\n",
    "<mark>\"some predefined error-function\" = il y en a une ou plusieurs d'error function ? Si une (ce que je pense), dire \"a\" au lieu de \"some\" et sinon mettre un s. </mark>\n",
    "\n",
    "*Source : [here](https://medium.com/analytics-vidhya/backpropagation-algorithm-using-pytorch-ee1287888aca)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi layer perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will build our neural net. Recall that MNIST is composed of images of size $28 \\times 28$, hence the dimenison of the input is $784$. We have $10$ classes, so the dimension of the output is $10$.\n",
    "\n",
    "In between we will insert $2$ hidden layers and use ReLU as our non-linearity (activation function).\n",
    "The first hidden layer is composed of $128$ neurons and the second one of $64$ neurons.\n",
    "\n",
    "We will not use GPU nor we will consider complicated neural nets in this TP. The goal is to introduce you to the basics without going into too complicated architechtures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleFeedForward(nn.Module):\n",
    "    def __init__(self, input_size=784, hidden_sizes=[128, 64],\n",
    "                 output_size=10):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_sizes[0]), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_sizes[1], output_size)\n",
    "        )\n",
    "             \n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, input_size)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we defined our neural net we need to train it.\n",
    "The training is going to be performed via Stochastic Gradient Descent evaluated on a mini batch of the data.\n",
    "That is, on the foward stage we will use not a single data point but several ones. In this case we set the size of mini batch equal to $32$.\n",
    "\n",
    "Actually, size of the mini batch, learning rate sizes of hidden layers are all considered as hyperparameters that can be finely tuned (some people even tune random seed, which is absolutely ridiculous). We will not talk about the hypeparameter tuning in this TP, to learn more have a look at https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html .\n",
    "\n",
    "\n",
    "**Important:** We do not require you to perform complicated hyperparameter tuning. This part is beyond the course. However, it is important that you can clearly write an architechture of a nerual net that you consider.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training consists of gradient steps over mini batch of data\n",
    "def train(model, trainloader, loss, optimizer, epoch, num_epochs):\n",
    "    # We enter train mode. This is useless for the linear model\n",
    "    # but is important for layers such as dropout, batchnorm, ...\n",
    "    model.train()\n",
    "    \n",
    "    loop = tqdm(trainloader)\n",
    "    loop.set_description(f'Training Epoch [{epoch + 1}/{num_epochs}]')\n",
    "    \n",
    "    # We iterate over the mini batches of our data\n",
    "    for inputs, targets in loop:\n",
    "    \n",
    "        # Erase any previously stored gradient\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #careful : the following line was a mistake in the teacher's proposition ! \n",
    "        #outputs = net(inputs) # mistake we need to replace net by the parameter \"model\"\n",
    "        outputs = model(inputs) # Forwards stage (prediction with current weights)\n",
    "        loss = criterion(outputs, targets) # loss evaluation\n",
    "        \n",
    "        loss.backward() # Back propagation (evaluate gradients) \n",
    "        \n",
    "        \n",
    "        # Making gradient step on the batch (this function takes care of the gradient step for us)\n",
    "        optimizer.step() \n",
    "        \n",
    "def validation(model, valloader, loss):\n",
    "    # Do not compute gradient, since we do not need it for validation step\n",
    "    with torch.no_grad():\n",
    "        # We enter evaluation mode.\n",
    "        model.eval()\n",
    "        \n",
    "        total = 0 # keep track of currently used samples\n",
    "        running_loss = 0.0 # accumulated loss without averagind\n",
    "        accuracy = 0.0 # accumulated accuracy without averagind (number of correct predictions)\n",
    "        \n",
    "        loop = tqdm(valloader) # This is for the progress bar\n",
    "        loop.set_description('Validation in progress')\n",
    "        \n",
    "        \n",
    "        # We again iterate over the batches of validation data. batch_size does not play any role here\n",
    "        for inputs, targets in loop:\n",
    "            # Run samples through our net\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Total number of used samples\n",
    "            total += inputs.shape[0]\n",
    "\n",
    "            # Multiply loss by the batch size to erase averagind on the batch\n",
    "            running_loss += inputs.shape[0] * loss(outputs, targets).item()\n",
    "            \n",
    "            # how many correct predictions\n",
    "            accuracy += (outputs.argmax(dim=1) == targets).sum().item()\n",
    "            \n",
    "            # set nice progress meassage\n",
    "            loop.set_postfix(val_loss=(running_loss / total), val_acc=(accuracy / total))\n",
    "        return running_loss / total, accuracy / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use again the MNIST dataset. This time we will use the official train/test split!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We download the oficial MNIST train set\n",
    "all_train = datasets.MNIST('data/',\n",
    "                           download=True,\n",
    "                           train=True,\n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "# We split the whole train set in two parts:\n",
    "# the one that we actually use for training\n",
    "# and the one that we use for validation\n",
    "batch_size = 32 # size of the mini batch\n",
    "num_train = int(0.8 * len(all_train))\n",
    "\n",
    "trainset, valset = torch.utils.data.random_split(all_train, [num_train, len(all_train) - num_train])\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions of the inputs are torch.Size([32, 1, 28, 28])\n",
      "The number on the image is: 9\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we can iterate over trainloader in the following way\n",
    "for inputs, targets in trainloader:\n",
    "    print(f'Dimensions of the inputs are {inputs.shape}')\n",
    "    plt.imshow(inputs[0][0], cmap='gray', interpolation='none')\n",
    "    print(f'The number on the image is: {targets[0]}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of ```inputs``` is $(32, 1, 28, 28)$. The first dimension indicates the size of the mini batch and is controlled by parameter ```batch_size```, the last two parameters are the 2D dimensions of the image and are equal to $28 \\times 28$ in case of the MNIST data. The lonely $1$, staying in the second dimension essentialy reflects the fact that the images are black and white. For instance, if MNIST were colored (there are variants of colored MNIST actually), then we would need $3$ (in case of RGB) colors to represent an image, thus $1$ would be replaced by $3$. \n",
    "\n",
    "**Question:** Run the above block several times. Is it plotting the same number all the time? If not, why?\n",
    "\n",
    "**Answer:** No, because ```torch.utils.data.random_split``` randomly split the dataset all_train into non-overlapping new datasets (trainset and valset).\n",
    "We can optionally fix the generator for reproducible results, e.g.:\n",
    "```trainset, valset = torch.utils.data.random_split(all_train, [num_train, len(all_train) - num_train], generator=torch.Generator().manual_seed(42))```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Net + training parameters\n",
    "num_epochs = 2 # how many passes over the whole train data\n",
    "input_size = 784 # flattened size of the image\n",
    "hidden_sizes = [128, 64] # sizes of hidden layers\n",
    "output_size = 10 # how many labels we have\n",
    "lr = 0.001 # learning rate\n",
    "momentum = 0.9 # momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing our model/loss/optimizer\n",
    "net = SimpleFeedForward(input_size, hidden_sizes, output_size) # Our neural net\n",
    "criterion = nn.CrossEntropyLoss() # Loss function to be optimized\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr, momentum=momentum) # Optimization algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba7fe92a128548bb891587677f48ce05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caa388a92f2f43c486bb0c2b5ed138a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=375.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aa1790793784e528c3ad9249c5f5753",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a41cf74025241239839cd43f24a1a7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=375.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# num_epochs indicates the number of passes over the data\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # makes one pass over the train data and updates weights\n",
    "    train(net, trainloader, criterion, optimizer, epoch, num_epochs)\n",
    "\n",
    "    # makes one pass over validation data and provides validation statistics\n",
    "    val_loss, val_acc = validation(net, valloader, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8013a9c7acb4d25bb67faccb24090fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=313.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy: 0.9826 | Test loss: 0.0521137018121779\n"
     ]
    }
   ],
   "source": [
    "# Let us evaluate our net on the test set that we have never seen!\n",
    "testset = datasets.MNIST('data/',\n",
    "                         download=True,\n",
    "                         train=False,\n",
    "                         transform=transforms.ToTensor())\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loss, test_acc = validation(net, testloader, criterion)\n",
    "print(f'Test accuracy: {test_acc} | Test loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Logistic regression via pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using above code as an example implement multinomial logistic regression and train it on the same data.\n",
    "For your report include:\n",
    "1. Mathematical description of logistic regression\n",
    "2. Mathematical description of optimization algorithm that you use\n",
    "3. High level idea of how to implement logisitic regression with pytorch\n",
    "4. Report classification accuracy on test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer :**\n",
    "\n",
    "**1. Mathematical description of logistic regression**\n",
    "\n",
    "Logistic Regression can be thought of as a simple, fully-connected neural network with one hidden layer.\n",
    "\n",
    "In the following network (see diagram), the **forward pass** are the three steps from the input to the output layer.\n",
    "\n",
    "<img src=\"https://aaronkub.com/images/logistic_regression_diagram.jpg\" width=\"500\" height=\"300\"/>\n",
    "\n",
    "1. The input data in blue are the $n$ features ($n =4$ in the example)\n",
    "2. In the hidden layer in red are the resulting matrix $Z$ of multiplying each m rows (training observations) of the dataset a weight matrix with n rows and k columns, with $k$ the number of unique classes you want to predict. We also add a bias to the result. The equation is $Z_{[m,k]} = X_{[m,n]} W_{[n,k]} + b_{[m,k]}$. $z_{i,j}$ is called the logit for the $j^{th}$ label of the $i^{th}$ training example\n",
    "\n",
    "Remark : To represent multinomial classes, we use **one-hot encoding** as we saw for TP2 : a simple transformation of a 1-dimensional vector of length $m$ into a binary tensor of shape $(m, k)$, where k is the number of unique classes. Each column in the new tensor represents a specific class label and for every row there is exactly one column with a 1, everything else is zero. But the PyTorch's loss function that we use (`nn.CrossEntropyLoss`) take directly class labels as their targets so we don't need to convert targets into onehot vectors. \n",
    "\n",
    "3. For the output layer in green, every logit of the matrix $Z$ are passed through an activation function called Softmax (equation below) and the results are numbers between 0 and 1. $a_{i,j}$ will correspond to the probability that observation $i$ is of the type $j$.\n",
    "\n",
    "\\begin{equation*}\n",
    "\\sigma_{i}(z_{i,j}) = \\frac{e^{z_{i,j}}}{\\sum_{n=0}^{k}e^{z_{i,n}}} = a_{i, j}\n",
    "\\end{equation*}\n",
    "\n",
    "=> Finally, we pick the node in the output layer that has the highest probability and choose that as the predicted class label.\n",
    "\n",
    "**2. Mathematical description of optimization algorithm that you use**\n",
    "\n",
    "At the end of each forward pass in the training process, we use the activations to determine the performance of the model using the cost called **Cross Entropy Loss** : $Cross\\:Entropy\\:Loss = -\\frac{1}{m}\\sum_{i=0}^{m} \\sum_{j=0}^{k} y_{i,j} \\cdot \\log (a_{i,j})$\n",
    "\n",
    "*Remark : To prevent the model from overfitting, we can also regularize the regression by simply adding a term to the cost function intended to penalize model complexity. We use : $L2\\:Regularized\\:Loss = -\\frac{1}{m}\\sum_{i=0}^{m} \\sum_{j=0}^{k} y_{i,j} \\cdot \\log (a_{i,j}) + \\lambda \\sum_{i=0}^{n} \\sum_{j=0}^{k} w_{i,j}^2$ with $\\lambda \\leq 0$ a hyperparameter to be tuned. We don't do it here because it is not specifically asked.*\n",
    "\n",
    "Finally, during the **backward pass**, go back through the network and make adjustments to every hidden layer’s parameters. The goal is to reduce the loss in the next training iteration. In this particular case of Logistic Regression, there’s only one layer of parameters that will get adjusted by a method called **Gradient Descent** : \n",
    "\n",
    "1. We first get the gradient of each model parameter using the backpropagation algorithm. \n",
    "\n",
    "$$\\nabla (W) =\n",
    "\\begin{bmatrix}\n",
    "\\nabla (w_{0,0}) & \\nabla (w_{0,1}) & \\cdots & \\nabla (w_{0,k}) \\\\\n",
    "\\nabla (w_{1,0}) & \\nabla (w_{1,1}) & \\cdots & \\nabla (w_{1,k}) \\\\\n",
    "\\vdots  & \\vdots  & \\cdots & \\vdots \\\\\n",
    "\\nabla (w_{n,0}) & \\nabla (w_{n,1}) & \\cdots & \\nabla (w_{n,k})\n",
    "\\end{bmatrix} \\qquad \\text{where} \\qquad \\nabla (w_{f,l}) = \n",
    "[\\frac{1}{m} \\sum_{i=0}^{m} \\left( x_{i,f} * (y_{i, l} - a_{i, l}) \\right)] + [2 \\lambda * w_{f,l}]$$\n",
    "\n",
    "$$\\nabla (b) = \n",
    "\\begin{bmatrix}\n",
    "\\nabla (b_{0}) & \\nabla (b_{1}) & \\cdots & \\nabla (b_{k})\n",
    "\\end{bmatrix} \\qquad \\text{where} \\qquad \\nabla (b_{l}) = \\frac{1}{m} \\sum_{i=0}^{m} \\left( y_{i, l} - a_{i, l} \\right)$$\n",
    "\n",
    "2. We update each model parameter in the opposite direction of its gradient.\n",
    "\n",
    "*Source : [here](https://aaronkub.com/2020/02/12/logistic-regression-with-pytorch.html) and [here](https://towardsdatascience.com/logistic-regression-on-mnist-with-pytorch-b048327f8d19)*\n",
    "\n",
    "\n",
    "**3. High level idea of how to implement logisitic regression with pytorch**\n",
    "\n",
    "**Preliminary remark:** In the function `train` constructed by the teacher, there was a little mistake that we had to correct so that the following code works. The following line...\n",
    "\n",
    "            #careful : the following line was a mistake in the teacher's proposition ! \n",
    "            #outputs = net(inputs) # mistake we need to replace net by the parameter \"model\"\n",
    "\n",
    "had been replaced by...\n",
    "\n",
    "            outputs = model(inputs) # Forwards stage (prediction with current weights)\n",
    "            \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add multinomial logistic regression\n",
    "class MultinomialLogisticRegression(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.classifier = torch.nn.Linear(input_size, output_size)\n",
    "        # We don't use Softmax here because the CrossEntropyLoss function computes softmax before the CE.\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, input_size)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing our model/loss/optimizer\n",
    "MLR = MultinomialLogisticRegression(input_size, output_size) \n",
    "criterion = nn.CrossEntropyLoss() # /!\\ computes softmax and then the cross entropy\n",
    "optimizer = optim.SGD(MLR.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0055a8ac56a34f4885ff9013b83f7b84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a79b32e680e43ed8236adfee2cdfa97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=375.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d22867cf1aa4c42b80b00b73609d308",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c4824d1836b41aaa1d5bb3472fca8f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=375.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# num_epochs indicates the number of passes over the data\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # makes one pass over the train data and updates weights\n",
    "    train(MLR, trainloader, criterion, optimizer, epoch, num_epochs)\n",
    "\n",
    "    # makes one pass over validation data and provides validation statistics\n",
    "    val_loss, val_acc = validation(MLR, valloader, criterion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Report classification accuracy on test data.**\n",
    "\n",
    "**=> The accuracy is 83 % with 2 epochs on test data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7153a0cefec4714bd3a51098ba208df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=313.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy: 0.83 | Test loss: 0.9829374759674072\n"
     ]
    }
   ],
   "source": [
    "# Let us evaluate our net on the test set that we have never seen!\n",
    "testset = datasets.MNIST('data/',\n",
    "                         download=True,\n",
    "                         train=False,\n",
    "                         transform=transforms.ToTensor())\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loss, test_acc = validation(MLR, testloader, criterion)\n",
    "print(f'Test accuracy: {test_acc} | Test loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elements of CNN: ```nn.Conv2d``` and ```MaxPool2d```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read this before starting: https://ttic.uchicago.edu/~shubhendu/Pages/Files/Lecture7_flat.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload the data\n",
    "all_train = datasets.MNIST('data/',\n",
    "                           download=True,\n",
    "                           train=True,\n",
    "                           transform=transforms.ToTensor())\n",
    "\n",
    "# We split the whole train set in two parts:\n",
    "# the one that we actually use for training\n",
    "# and the one that we use for validation\n",
    "batch_size = 32 # size of the mini batch\n",
    "num_train = int(0.8 * len(all_train))\n",
    "\n",
    "trainset, valset = torch.utils.data.random_split(all_train, [num_train, len(all_train) - num_train])\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Net + training parameters\n",
    "num_epochs = 2 # how many passes over the whole train data\n",
    "input_size = 784 # flattened size of the image\n",
    "hidden_sizes = [128, 64] # sizes of hidden layers\n",
    "output_size = 10 # how many labels we have\n",
    "lr = 0.001 # learning rate\n",
    "#momentum = 0.9 # momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding the convolutional layer in pytorch**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we instanciate ```nn.Conv2d(1, 1, kernel_size=2, stride=[1, 1], padding=0)``` it has a parameter ```weight``` which precisely describes the kernel used for our convolution. In the beginning it is initialized randomly, and our goal is to eventually learn its weights (as usual via backpropagation!).\n",
    "Before building our first CNN let us have a look at the kernel and what it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[-0.32727212,  0.08143073],\n",
       "         [-0.4342568 ,  0.35001594]]]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 input channel (first 1 in nn.Conv2d)\n",
    "# 1 output channel (second 1 in nn.Conv2d)\n",
    "# 2x2 kernel (kernel_size=2)\n",
    "# the kernel slides by one step in (x, y) direction (stride=[1, 1])\n",
    "# we do not augment the picture with white borders (padding=0)\n",
    "conv = nn.Conv2d(1, 1, kernel_size=2, stride=[1, 1], padding=0) \n",
    "# Get kernel value.\n",
    "weight = conv.weight.data.numpy()\n",
    "weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualization.** We will plot the initial image, the kernel, and the resulting image. In order to understand what is going on, the resulting image will be computed in two ways. First of all it will be computed by using ```conv1(image)```. Secondly, we will manually apply the sliding kernel to each $2\\times 2$ window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'By hand')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# take one image\n",
    "image, _ = next(iter(trainloader))\n",
    "fig, axs = plt.subplots(1, 4)\n",
    "fig.tight_layout()\n",
    "fig.suptitle('Convolution')\n",
    "\n",
    "# plot the image\n",
    "axs[0].imshow(image[0][0], cmap='gray', interpolation='none')\n",
    "axs[0].set_title('Original image')\n",
    "\n",
    "# plot the kernel\n",
    "axs[1].imshow(weight[0][0], cmap='gray', interpolation='none')\n",
    "axs[1].set_title('2x2 kernel')\n",
    "\n",
    "# plot resulting image\n",
    "axs[2].imshow(conv(image)[0][0].detach().numpy(), cmap='gray', interpolation='none')\n",
    "axs[2].set_title('Resulting image')\n",
    "\n",
    "# Making the same by hands\n",
    "# IMPORTANT: we strongly suggest to understand the below code\n",
    "np_image = image[0][0].data.numpy() # get numpy image\n",
    "image_convolved = np.zeros((27, 27)) # here we store our result\n",
    "for i in range(27):\n",
    "    for j in range(27):\n",
    "        image_convolved[i, j] = np.sum(np_image[i:i+2, j:j+2] * weight) # apply the kernel for each 2x2 window\n",
    "        \n",
    "axs[3].imshow(image_convolved, cmap='gray', interpolation='none')\n",
    "axs[3].set_title('By hand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28)\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n",
      "(28, 28)\n",
      "i: 10  j: 10\n",
      "3 carrés adjacent au pixel considéré : np_image[i:i+2, j:j+2]: [[0.         0.16078432]\n",
      " [0.         0.44313726]]\n",
      "i: 10  j: 11\n",
      "3 carrés adjacent au pixel considéré : np_image[i:i+2, j:j+2]: [[0.16078432 0.9529412 ]\n",
      " [0.44313726 0.99215686]]\n",
      "i: 11  j: 10\n",
      "3 carrés adjacent au pixel considéré : np_image[i:i+2, j:j+2]: [[0.         0.44313726]\n",
      " [0.32156864 0.9137255 ]]\n",
      "i: 11  j: 11\n",
      "3 carrés adjacent au pixel considéré : np_image[i:i+2, j:j+2]: [[0.44313726 0.99215686]\n",
      " [0.9137255  0.9882353 ]]\n"
     ]
    }
   ],
   "source": [
    "# Zoom to understand the code\n",
    "np_image = image[0][0].data.numpy() # get numpy image\n",
    "print(np_image.shape) #28x28\n",
    "print(np_image[2]) #valeur des pixels de la 4e ligne !\n",
    "image_convolved = np.zeros((28, 28)) #on fait une image (27x27? => 28x28) avec que des 0. \n",
    "print(image_convolved.shape)\n",
    "for i in range(28):\n",
    "    for j in range(28):\n",
    "        if((j==10 and i==11) or (j==10 and i==10) or (j==11 and i==11) or(j==11 and i==10) ):\n",
    "            print(\"i:\",i,\" j:\",j)\n",
    "            print(\"3 carrés adjacent au pixel considéré : np_image[i:i+2, j:j+2]:\",np_image[i:i+2, j:j+2])\n",
    "        image_convolved[i, j] = np.sum(np_image[i:i+2, j:j+2] * weight) # apply the kernel for each 2x2 window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'By hand')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(1, 2)\n",
    "axs[0].imshow(image[0][0], cmap='gray', interpolation='none')\n",
    "axs[0].set_title('Original image')\n",
    "axs[1].imshow(image_convolved, cmap='gray', interpolation='none')\n",
    "axs[1].set_title('By hand')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem.** Provide 'by hand' implementation of the following kernel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1 input channel (first 1 in nn.Conv2d)\n",
    "# 1 output channel (second 1 in nn.Conv2d)\n",
    "# 4x4 kernel (kernel_size=4) (before it was 2x2)\n",
    "# the kernel slides by 3 step in (x, y) direction (stride=[4, 4]) (before : 1 step stride [1,1])\n",
    "# we do not augment the picture with white borders (padding=0)\n",
    "\n",
    "conv = nn.Conv2d(1, 1, kernel_size=4, stride=[4, 4], padding=0) \n",
    "# Get kernel value.\n",
    "weight = conv.weight.data.numpy()\n",
    "\n",
    "# take one image\n",
    "image, _ = next(iter(trainloader))\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 3) #correction (1,4) before\n",
    "fig.tight_layout()\n",
    "fig.suptitle('Convolution')\n",
    "\n",
    "# plot the image\n",
    "axs[0].imshow(image[0][0], cmap='gray', interpolation='none')\n",
    "axs[0].set_title('Original image')\n",
    "\n",
    "# plot the kernel\n",
    "axs[1].imshow(weight[0][0], cmap='gray', interpolation='none')\n",
    "axs[1].set_title('4x4 kernel')\n",
    "\n",
    "# plot resulting image\n",
    "axs[2].imshow(conv(image)[0][0].detach().numpy(), cmap='gray', interpolation='none')\n",
    "axs[2].set_title('Resulting image')\n",
    "\n",
    "# Making the same by hands\n",
    "# PROBLEM: FILL IN THIS PART. \n",
    "np_image = image[0][0].data.numpy() # get numpy image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Understanding the pooling layer in pytorch**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max pooling is what often used in practice, it amounts to picking only the largest value of a pixel in a given window. In pytorch it is done via ```MaxPool2d(kernel_size=k, stride=s)```, which has two parameters: kernel size and the stride. Note that there are no weights to learn here, so this layer is simply fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'By hand')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# kernel_size -- size of the max pool window\n",
    "pool = nn.MaxPool2d(kernel_size=4, stride=[4,4])\n",
    "\n",
    "fig, axs = plt.subplots(1, 3)\n",
    "fig.tight_layout()\n",
    "fig.suptitle('Pooling')\n",
    "\n",
    "# plot the image\n",
    "axs[0].imshow(image[0][0], cmap='gray', interpolation='none')\n",
    "axs[0].set_title('Original image')\n",
    "\n",
    "\n",
    "# plot resulting image\n",
    "axs[1].imshow(pool(image)[0][0].detach().numpy(), cmap='gray', interpolation='none')\n",
    "axs[1].set_title('Resulting image')\n",
    "\n",
    "# Making the same by hands\n",
    "# IMPORTANT: we strongly suggest to understand the below code\n",
    "np_image = image[0][0].data.numpy() # get numpy image\n",
    "image_pooled = np.zeros((7, 7)) # here we store our result\n",
    "for i in range(0, 27, 4):\n",
    "    for j in range(0, 27, 4):\n",
    "        image_pooled[int(i / 4), int(j / 4)] = np.max(np_image[i:i+4, j:j+4]) # max pooling\n",
    "        \n",
    "axs[2].imshow(image_pooled, cmap='gray', interpolation='none')\n",
    "axs[2].set_title('By hand')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bulding a simple ConvNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=5, stride=[1, 1], padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(14 * 14 * 8, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 10),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the first layer is ```nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2)```, the parameters here are chosen in such a way that the size of each output channel remains as $28 \\times 28$. Indeed, setting ```padding = 2``` we augmented our initial image to $32 \\times 32$, then we slide a kernel of size $5 \\times 5$ by $1$ in both $(x, y)$ directions which result in a $28 \\times 28$ output image (and $8$ channels).\n",
    "\n",
    "In general the formula for square images and squared kernels is\n",
    "$$\n",
    "    S_{out} = \\frac{S_{in} - S_{kernel} + 2S_{padding}}{S_{stride}} + 1\n",
    "$$\n",
    "\n",
    "In our case it is\n",
    "\n",
    "$$\n",
    "    S_{out} = \\frac{28 - 5 + 4}{1} + 1 = 28\n",
    "$$\n",
    "\n",
    "Then the output of ```nn.Conv2d(1, 8, kernel_size=5, stride=1, padding=2)``` goes into ```nn.ReLU()``` our favorite non-linearity and eventually into the pooling layer ```nn.MaxPool2d(kernel_size=2, stride=2)```.\n",
    "The ```nn.ReLU()``` doe not affect the size, hence ```nn.MaxPool2d(kernel_size=2, stride=2)``` receives $8$ channels of $28 \\times 28$ images as computed above.\n",
    "\n",
    "```nn.MaxPool2d(kernel_size=2, stride=2)``` will be applied to each single channel, with ```kernel_size=2, stride=2``` meaning that the output will still have $8$ channels but the images will be halfed in both $(x, y)$ directions. Hence the output of ```nn.MaxPool2d(kernel_size=2, stride=2)``` has $8$ channels with $14 \\times 14$ images.\n",
    "\n",
    "After all this, we will flatten our features and put the into simple ```nn.Linear(14 * 14 * 8, 500)```, where the input size is precisely the output size of ```nn.MaxPool2d(kernel_size=2, stride=2)```, and $500$ stands for the output size of this linear layer.\n",
    "Finally, we apply our favorite nonlinearity to ```nn.Linear(14 * 14 * 8, 500)``` followed by fully connected linear layer ```nn.Linear(500, 10)``` to match the dimension of $10$ classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a21d49dd1c6e4f6e9b6452d2854ccb1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47d20a133ebc4c2f83e611ac3ebf6cc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=375.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dffa5010154947c6b32c07cf27124350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e91c93a1c7245cab4c91b88ea018c04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=375.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "net = ConvNet()\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # makes one pass over the train data and updates weights\n",
    "    train(net, trainloader, criterion, optimizer, epoch, num_epochs)\n",
    "\n",
    "    # makes one pass over validation data and provides validation statistics\n",
    "    val_loss, val_acc = validation(net, valloader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d595dde64da949faaa3738e00c13464d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=313.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy: 0.9826 | Test loss: 0.052113702251017094\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = validation(net, testloader, criterion)\n",
    "print(f'Test accuracy: {test_acc} | Test loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see the result here is much better, than in the simple multilayer perceptron. But note, we have actualy trained muuuuuch more parameters here and, at least on my computer, it takes considerably more time.\n",
    "\n",
    "Here you can see the summary of current state of the art results on MNIST: https://www.kaggle.com/c/digit-recognizer/discussion/61480\n",
    "\n",
    "As you see our score barely beats a carefully built random forest or **kNN**! To get extra $0.01$ requires much more fine tuning, which is of course is not the goal here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the code for ConvNet and insert Dropout layer (whereever you want).\n",
    "\n",
    "Include in your report:\n",
    "1. High level description of the dropout\n",
    "2. High level description of your architechture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "**1. High level description of the dropout**\n",
    "\n",
    "The idea of \"Dropout\" is to prevent from overfitting in deep neural network with large parameters on the data. As you can see in the following figure, \"Dropout\" deactivates the neurons randomly at each training step instead of training the data on the original network, we train the data on the network with dropped out nodes. In the next iteration of the training step, the hidden neurons which are deactivated by dropout changes because of its probabilistic behavior. In this way, by applying dropout i.e…deactivating certain individual nodes at random during training we can simulate an ensemble of neural network with different architectures.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/612/1*S-Rr9boTfKusUzETeKW6Mg.png\" width=\"500\" height=\"300\"/>\n",
    "\n",
    "In Pytorch, we simply need to introduce `nn.Dropout` layers specifying the rate at which to drop (i.e. zero) units. Learning a neural network with dropout is usually slower than without dropout. \n",
    "\n",
    "*Source : [here](https://towardsdatascience.com/batch-normalization-and-dropout-in-neural-networks-explained-with-pytorch-47d7a8459bcd)*\n",
    "\n",
    "**2. High level description of your architechture**\n",
    "\n",
    "\n",
    "<mark>TODO : décrire qu'on a testé plusieurs types de modèles et décrire l'architecture de chacun d'eux ainsi que l'accuracy obtenu. L'idéal serait d'accompagner cela d'un graphique pour montrer l'amélioration de l'architecture que nous proposons (diminution de l'overfitting par ex...)</mark>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark> @Isabelle. J'ai testé plusieurs modèles convolutifs avec dropout. Les résultats chiffré sont dans le dernier paragraphe de Bilan avec des suggestions sur les modèles à conserver ou non. Ce que je vois qu'il rester à faire par exemple : </mark>\n",
    "\n",
    "1. <mark>Finir la rédaction de cette partie en répondant à la partie 2. High level description of your architechture</mark>\n",
    "\n",
    "2. <mark>Faire (éventuellement, si pertinent) un graphique en comparant Conv avec Conv4/Conv5 ? pour montrer pédagogiquement la baisse de l'overfitting permise par Conv4/Conv5 ? Je pense que faire ce graphique n'est pas une mince affaire car il demande à modifier le code. J'ai mis dans mon brouillon quelques sources d'inspiration pour faire des graphiques et quelques pistes en vrac... Mais la plupart des graphiques que je vois mettent les epoch en abscisse or nous on a que 2 epochs dans nos tests donc je ne sais pas si c'est adapté... A voir... </mark>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On teste juste d'ajouter dropout de 0.5 avant la couche linéaire \n",
    "class ConvNet2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=5, stride=[1, 1], padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5), #NEW DROPOUT = 50 % probability \n",
    "            nn.Linear(14 * 14 * 8, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspiré de version Vanilla d'ici :\n",
    "#https://github.com/jeremyfix/deeplearning-lectures/blob/master/LabsSolutions/00-pytorch-FashionMNIST/models.py\n",
    "class ConvNet3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Sequential( #before = layer1\n",
    "            nn.Conv2d(1, 16,kernel_size=5,stride=1,padding=int((5-1)/2), bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(16, 32,kernel_size=5,stride=1,padding=int((5-1)/2), bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(32, 64,kernel_size=5,stride=1,padding=int((5-1)/2), bias=True),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        \n",
    "        probe_tensor = torch.zeros((1,1,28,28))\n",
    "        out_features = self.layer1(probe_tensor).view(-1)\n",
    "        num_classes = 10 \n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(out_features.shape[0], 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspiré de version d'ici :\n",
    "#https://github.com/nagadakos/ml-repo/blob/master/PyTorch/KerasMirror_Pytorch.py\n",
    "class ConvNet4(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        num_classes = 10 \n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.drop = nn.Dropout2d(p=0.25)\n",
    "        self.fc1 = nn.Linear(64*144, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x), 2)\n",
    "        x = self.drop(F.max_pool2d(F.relu(self.conv2(x)), 2))\n",
    "        # x = self.drop(x)\n",
    "        x = x.view(-1, 64*144)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspiré de version Fancy d'ici :\n",
    "#https://github.com/jeremyfix/deeplearning-lectures/blob/master/LabsSolutions/00-pytorch-FashionMNIST/models.py\n",
    "\n",
    "def conv_bn_relu(in_channels, out_channels, ks):\n",
    "    return [nn.Conv2d(in_channels, out_channels,\n",
    "                  kernel_size=ks,\n",
    "                  stride=1,\n",
    "                  padding=int((ks-1)/2), bias=True),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True)]\n",
    "    \n",
    "class ConvNet5(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        num_classes = 10\n",
    "        base_n = 64\n",
    "        self.features = nn.Sequential(\n",
    "            *conv_bn_relu(1, base_n, 3),\n",
    "            *conv_bn_relu(base_n, base_n, 3),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Dropout(0.4),\n",
    "            *conv_bn_relu(base_n, 2*base_n, 3),\n",
    "            *conv_bn_relu(2*base_n, 2*base_n, 3),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Dropout(0.4),\n",
    "            *conv_bn_relu(2*base_n, 4*base_n, 3),\n",
    "            *conv_bn_relu(4*base_n, 4*base_n, 3),\n",
    "            nn.AvgPool2d(kernel_size=7)\n",
    "        )\n",
    "\n",
    "        self.lin1 = nn.Linear(4*base_n, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x  = self.features(x)\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        y = self.lin1(nn.functional.dropout(x, 0.5, self.training, inplace=True))\n",
    "        #y = self.classifier(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18db04be97444a8a845ce82017dc0d2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "641f181c47c947009a206008faf57e36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=375.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcc53bf471d848588abe89afc16e035f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1500.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f4ebf36612545ac82638ce0a51db3fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=375.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "net = ConvNet5() #test the model of our choice ConvNetX (2 to 5)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # makes one pass over the train data and updates weights\n",
    "    train(net, trainloader, criterion, optimizer, epoch, num_epochs)\n",
    "\n",
    "    # makes one pass over validation data and provides validation statistics\n",
    "    val_loss, val_acc = validation(net, valloader, criterion)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc2474da15294a82853d0baea9e6bc4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=313.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy: 0.9912 | Test loss: 0.027332775755273177\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = validation(net, testloader, criterion)\n",
    "print(f'Test accuracy: {test_acc} | Test loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=> **Bilan actuel des modèles qui ont tourné (tous sur 2 epochs) :**\n",
    "\n",
    "- Avec Conv : 98,3 % d'accuracy et très rapide => baseline\n",
    "- Avec Conv2 : 98,3 % d'accuracy et rapide => Conserver à but pédagogique et dire qu'on va ensuite compliquer un peu le modèle\n",
    "- Avec Conv3 : 98,9 % d'accuracy et lent => Conserver ? bon compromis entre vitesse et accuracy ? Car moins lent que Conv4 ? \n",
    "- Avec Conv4 : 98,4 % d'accuracy et lent => Ne pas conserver\n",
    "- Avec Conv5 : 99,1 % d'accuracy et trèèèèès lent (40min)  => Conserver = meilleur modèle jusqu'à présent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the succesful completion of this TP, we expect you to be able to understand the architectures of NN, CNN.\n",
    "For instance, have a look at the famous AlexNet https://github.com/pytorch/vision/blob/master/torchvision/models/alexnet.py and see if you can understand its architechture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tout ce qu'il y a ci-dessous : Brouillon Kim à supprimer plus tard\n",
    "\n",
    "Let's calculate the critical points of the function $f$ which are such as $\\frac{\\partial f}{\\partial w_1}(w) = \\dots = \\frac{\\partial f}{\\partial w_5}(w) = 0$\n",
    "\n",
    "with $\\forall i \\in \\{1, \\dots, 5\\}$, $\\frac{\\partial f}{\\partial w_i}(w) = -2 (1-\\sum_{k=1}^5 w_k$). So the critical points are all the points $w=(w_1, \\dots, w_5)$ such that $\\sum_{k=1}^5 w_k = 1$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**References qui peuvent servir**\n",
    "\n",
    "Isabelle: \n",
    "\n",
    "Exemples de code que lequel je me suis inspirée mais en restant proche du code du prof car ces codes diffèrent énormément : https://github.com/liaison/Multinomial-Logistic-Regression/blob/master/MNL.py et : https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/01-basics/logistic_regression/main.py\n",
    "\n",
    "\n",
    "Kim:\n",
    "\n",
    "HI, it depends on your loss function, but some PyTorch’s loss functions take class labels as their targets(e.g. NLLloss 3.8k). So if you use them, you don’t need to convert targets into onehot vectors.\n",
    "https://www.programcreek.com/python/example/105103/torchvision.datasets.MNIST\n",
    "\n",
    "https://nextjournal.com/gkoehler/pytorch-mnist\n",
    "\n",
    "=> https://towardsdatascience.com/logistic-regression-on-mnist-with-pytorch-b048327f8d19 tentative solution\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tests pour les graphiques : non aboutis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation2(model, valloader, loss):\n",
    "    # Do not compute gradient, since we do not need it for validation step\n",
    "    with torch.no_grad():\n",
    "        # We enter evaluation mode.\n",
    "        model.eval()\n",
    "        \n",
    "        total = 0 # keep track of currently used samples\n",
    "        running_loss = 0.0 # accumulated loss without averagind\n",
    "        accuracy = 0.0 # accumulated accuracy without averagind (number of correct predictions)\n",
    "        \n",
    "        loop = tqdm(valloader) # This is for the progress bar\n",
    "        loop.set_description('Validation in progress')\n",
    "        test_acc_vec = []\n",
    "        test_loss_vec = [] \n",
    "        \n",
    "        # We again iterate over the batches of validation data. batch_size does not play any role here\n",
    "        for inputs, targets in loop:\n",
    "            # Run samples through our net\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Total number of used samples\n",
    "            total += inputs.shape[0]\n",
    "\n",
    "            # Multiply loss by the batch size to erase averagind on the batch\n",
    "            running_loss += inputs.shape[0] * loss(outputs, targets).item()\n",
    "            \n",
    "            # how many correct predictions\n",
    "            accuracy += (outputs.argmax(dim=1) == targets).sum().item()\n",
    "            \n",
    "            #NEW\n",
    "            test_loss = running_loss / total\n",
    "            test_acc = accuracy / total\n",
    "            test_loss_vec.append(test_loss)\n",
    "            test_acc_vec.append(test_acc)\n",
    "            \n",
    "            # set nice progress meassage\n",
    "            loop.set_postfix(val_loss=(running_loss / total), val_acc=(accuracy / total))\n",
    "                       \n",
    "        return test_loss, test_acc, test_loss_vec, test_acc_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Conv', \n",
    "          'Conv2']\n",
    "\n",
    "plt.figure(figsize=(8, 7))\n",
    "plt.plot(range(1, len(r)+1), r, '.-', label=\"Conv\", alpha=0.6);\n",
    "plt.ylim([50, 250]);\n",
    "plt.legend(loc=1);\n",
    "plt.xlabel('iteration');\n",
    "plt.ylabel('Accuracy in test set');\n",
    "plt.title('Accuracy on MNIST dataset with the different Convolutional networks')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the results of the models if it is long to implement. \n",
    "import pickle\n",
    "with open(\"data/sauv_proj/test_loss_vec_conv5.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(test_loss_vec, fp)\n",
    "with open(\"data/sauv_proj/test_acc_vec_conv5.txt\", \"wb\") as fp:   #Pickling\n",
    "    pickle.dump(test_acc_vec, fp)\n",
    "with open(\"data/sauv_proj/test_acc_vec_conv5.txt\", \"rb\") as fp:   # Unpickling\n",
    "    b = pickle.load(fp)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compléter les recherches sur les réseaux convolutifs avec des éléments piochés dans cette biblio ?**\n",
    "\n",
    "https://www.kaggle.com/c/digit-recognizer/discussion/61480 (ici ils disent les modèles les plus performants notamment celui qui atteint 99 % avec un dropout) => essayer de le reproduire. \n",
    "\n",
    "https://teaching.pages.centralesupelec.fr/deeplearning-lectures-build/00-pytorch-fashionMnist.html\n",
    "\n",
    "https://nagadakos.github.io/2018/09/23/dropout-effect-discussion/\n",
    "\n",
    "https://github.com/jeremyfix/deeplearning-lectures/blob/master/LabsSolutions/00-pytorch-FashionMNIST/data.py (code des solutions)\n",
    "\n",
    "https://xuwd11.github.io/Dropout_Tutorial_in_PyTorch/ (pour rajouter un graphique de l'amélioration de l'overfitting)\n",
    "\n",
    "https://www.kaggle.com/cdeotte/how-to-choose-cnn-architecture-mnist (autre exemples de graphiques)\n",
    "\n",
    "https://github.com/nagadakos/ml-repo/blob/master/PyTorch/KerasMirror_Pytorch.py ajouter history dans les codes pour faire les plots\n",
    "\n",
    "\"In their original paper, Srivastava et al. 2014 suggest for example Dropping out 20% of the input units and 50% of the hidden units was often found to be optimal. \""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
