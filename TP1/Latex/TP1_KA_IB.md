**NAMES :  Kim ANTUNEZ, Isabelle BERNARD (Group : Mr Denis)**

<h1><center> TP1 : Basic functions for Supervised Machine Learning. </center></h1>

The deadline for report submission is Tuesday, November 10th 2020.

Note: the goal of this first TP is to become familiar with 'sklearn' class in Python. In particular, we introduce most popular supervised learning algorithms. 

PART 1 is a list of commands that should be followed step by step. PART 2 is an open problem for which we are waiting for your creativity!

## Imported packages


```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.svm import LinearSVC
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import MaxAbsScaler, StandardScaler
from sklearn.metrics import accuracy_score, balanced_accuracy_score, make_scorer, confusion_matrix


%matplotlib notebook
```


```python
import random
random.seed(1) #to fix random and have the same results for both of us 
```

#  PART 1 -- MNIST


In the first part of TP1 we pursue the following goals:
1. Apply standard ML algorithms on a standard benchmark data
2. Learn basic means of data visualizations
3. Get familiar with sklearn's GridSearchCV and Pipeline

## Loading the data

MNIST dataset consists of black and white images of hand-written digits from $0$ to $9$ of size $28 \times 28$.
In this exercise we will work with a small from the original MNIST dataset. 

If you are interested in the whole dataset, execute the following commands
```python
from sklearn.datasets import fetch_mldata
mnist = fetch_mldata('MNIST original', data_home=custom_data_home)
```

Hence, the observations $(X_1, Y_1), \ldots, (X_n, Y_n)$ are such that $X_i \in \mathbb{R}^{784}$ and $Y_i \in \{0, \ldots, 9\}$. To be more precise, each component of vector $X_i$ is a number between $0$ and $255$, which signifies the intensity of black color.

The initial goal is to build a classifier $\hat g$, which receives a new image $X$ and outputs the number that is present on the image.


```python
X_train = np.load('data/mnist1_features_train.npy', allow_pickle=True)
y_train = np.load('data/mnist1_labels_train.npy', allow_pickle=True)
X_test = np.load('data/mnist1_features_test.npy', allow_pickle=True)
y_test = np.load('data/mnist1_labels_test.npy', allow_pickle=True)

n_samples, n_features = X_train.shape # extract dimensions of the design matrix
print('Train data contains: {} samples of dimension {}'.format(n_samples, n_features))
print('Test data contains: {} samples'.format(X_test.shape[0]))
```

    Train data contains: 2000 samples of dimension 784
    Test data contains: 200 samples
    

## Looking at the data

Since each observation is actually an image, we can visualize it.


```python
axes = plt.subplots(1, 10)[1]  # creates a grid of 10 plots

# More details about zip() function here 
# https://docs.python.org/3.3/library/functions.html#zip
images_and_labels = list(zip(X_train, y_train)) 
for ax, (image, label) in zip(axes, images_and_labels[:10]):
    ax.set_axis_off()
    ax.imshow(image.reshape((28, 28)), cmap=plt.cm.gray_r, interpolation='nearest')
    ax.set_title('{}'.format(label))
```


    <IPython.core.display.Javascript object>



<img src="data:image/png;base64,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" width="640">



```python
for i in range(10):
    print('Number of {}s in the train dataset is {}'.format(i, np.sum([y_train == str(i)])))
```

    Number of 0s in the train dataset is 196
    Number of 1s in the train dataset is 226
    Number of 2s in the train dataset is 214
    Number of 3s in the train dataset is 211
    Number of 4s in the train dataset is 187
    Number of 5s in the train dataset is 179
    Number of 6s in the train dataset is 175
    Number of 7s in the train dataset is 225
    Number of 8s in the train dataset is 186
    Number of 9s in the train dataset is 201
    

From the above we conclude that the dataset is rather balanced, that is, each class contains similar amount of observations. The rarest class is $y = 6$ with $175$ examples and the most common class is $y = 1$ with $226$ examples

## Cross-validation with GridSearchCV

**Question:** Explain in your report what happens when we run ```clf.fit(X_train, y_train)```
What is the complexity for each of the three following cases? 

**Answer :** The general objective here is to obtain a first classifier with the **KNN method**. To do that, we test different parameters of the KNN methods and choose the bests using a **cross validation**. That is to say that we test the KNN method by varying the number of neighbors from 1 to 5. The cross validation method used is called **the 3-fold Cross Validation** (CV) following those different steps:
1. we divide our training sample into 3 training sub-samples
2. we train the model on 2 samples and test it on the third one
3. we choose the parameter which has the best average test accuracy (see definition later) on the 3 samples.

**clf.fit(X_train, y_train)** applies what is described above to the training sample. It fits the model (learns from it) using X_train as training data and y_train as target values. The first clf (for classifier) used here is "KNeighborsClassifier" that is to say the k-nearest neighbors vote.

Let's imagine that you train a model on n points and it takes x minutes. If you train it on kn points, it takes kx minutes if the training time is linear, but sometimes it is more. For example, if it takes k2x, the training time is quadratic in the number of points. That is what we call the **complexity** of an algorithm. The question here is very broad (not very precise), because there are at least two different kinds of complexities : **training complexity and prediction complexity**. 

We define the complexity using a Big-O measure. It provides us with an asymptotic upper bound for the growth rate of the runtime of the chosen algorithm. Calling n the number of training samples and p the number of features the complexity predicted for the three methods are : 

* For the **knn** classifier : The parameter used for the algorithm is here ‘auto’. It selects ‘kd_tree’ if $k < N/2$ and the ‘effective_metric_’ is in the ‘VALID_METRICS’ list of ‘kd_tree’. It selects ‘ball_tree’ if k < N/2 and the ‘effective_metric_’ is not in the ‘VALID_METRICS’ list of ‘kd_tree’. It selects ‘brute’ if k >= N/2. For the brute-force method there is no training, (training complexity = $O(1)$),  but classifying has a high cost ($O(knp)$). kd-tree and ball_tree are $O(pnlog(n))$ for training and $O(klog(n))$ for prediction. 
. See precisions [here](https://towardsdatascience.com/k-nearest-neighbors-computational-complexity-502d2c440d5) and [here](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html). 

* Support Vector Machines (**SVM**) are powerful tools, but their compute and storage requirements increase rapidly with the number of training vectors. The core of an SVM is a quadratic programming problem (QP), separating support vectors from the rest of the training data. The QP solver used by the libsvm-based implementation (`SVC` function) scales between O($pn^2$) and O($pn^3$) depending on how efficiently the libsvm cache is used in practice (dataset dependent). But recent approaches like [this one](https://www.cs.huji.ac.il/~shais/papers/SSSICML08.pdf) are inverse in the size of the training set. In the case of the `LinearSVC` method used in this "TP", it is indicated in the [documentation](https://scikit-learn.org/stable/modules/svm.html#complexity) that the implementation is much more efficient than its libsvm-based `SVC` counterpart and it's training complexity is $O(pn)$ and prediction one remains $O(n_{sv}*p)$ with $n_{sv}$ the number of Support Vectors.

* For **logistic regressions**, training complexity is $O(np)$ and prediction one is $O(p)$. See proof [here](https://levelup.gitconnected.com/train-test-complexity-and-space-complexity-of-logistic-regression-2cb3de762054).

*Main sources : [here](https://medium.com/@paritoshkumar_5426/time-complexity-of-ml-models-4ec39fad2770) and [here](https://www.thekerneltrip.com/machine/learning/computational-complexity-learning-algorithms/)*.


```python
# GridSearchCV with kNN : a simple baseline
knn = KNeighborsClassifier() # defining classifier
parameters = {'n_neighbors': [1, 2, 3, 4, 5]} # defining parameter space
```


```python
clf = GridSearchCV(knn, parameters, cv=3) #cross-validation : method 3-fold.
```


```python
clf.fit(X_train, y_train) #
```




    GridSearchCV(cv=3, error_score=nan,
                 estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30,
                                                metric='minkowski',
                                                metric_params=None, n_jobs=None,
                                                n_neighbors=5, p=2,
                                                weights='uniform'),
                 iid='deprecated', n_jobs=None,
                 param_grid={'n_neighbors': [1, 2, 3, 4, 5]},
                 pre_dispatch='2*n_jobs', refit=True, return_train_score=False,
                 scoring=None, verbose=0)




```python
print('Returned hyperparameter: {}'.format(clf.best_params_))
print('Best classification accuracy in train is: {}'.format(clf.best_score_))
print('Classification accuracy on test is: {}'.format(clf.score(X_test, y_test)))
```

    Returned hyperparameter: {'n_neighbors': 1}
    Best classification accuracy in train is: 0.891497944721333
    Classification accuracy on test is: 0.875
    


```python
print(confusion_matrix(y_test, clf.predict(X_test)))
```

    [[21  0  0  0  0  0  1  0  0  0]
     [ 0 26  0  0  0  0  0  0  0  0]
     [ 0  0 14  0  0  2  0  0  0  0]
     [ 0  0  0 19  0  2  0  0  1  1]
     [ 0  1  0  0 17  0  0  0  0  2]
     [ 0  0  0  0  1  7  1  0  1  0]
     [ 0  0  0  0  0  1 23  0  0  0]
     [ 0  0  0  0  1  0  0 15  0  0]
     [ 0  1  0  1  0  0  0  0 14  1]
     [ 1  1  0  0  2  0  0  3  0 19]]
    

**Question:** What is the test accuracy? What would be the accuracy of random guess?

**Answer :**
Accuracy is the number of correctly predicted data points out of all the data points. It is used to determine which model is best at identifying relationships and patterns between variables in a dataset based on the input data or training data. The `accuracy_score` function computes the accuracy, either the fraction (default) or the count (`normalize=False`) of correct predictions.

More formally, for a **binary problem** it is defined as followed.
$$
\texttt{accuracy} = \frac{TP + TN}{TP + TN + FP + FN}
$$

With the following definitions:


|  | Actual : Class 1                                                     | Actual : Class 0                                                     |   |   |
|--------------------------|-------------------------------------------------------------|-------------------------------------------------------------|---|---|
| Predicted : Class 1                  | TP (True Positive) Y_true and Y_pred in Class 1             | FP (False Positive) Y_true in Class 0 and Y_pred in Class 1 |   |   |
| Predicted : Class 0                  | FN (False Negative) Y_true in Class 1 and Y_pred in Class 0 | TN (True Negative) Y_true and Y_pred in Class 0             |   |   |


In **multilabel classification**, the function returns the subset accuracy. If the entire set of predicted labels for a sample strictly match with the true set of labels, then the subset accuracy is 1; otherwise it is 0.

If $\hat{y}_i$ is the predicted value of the $i$-th sample and $y_i$ is the corresponding true value, then the fraction of correct predictions over is defined as $n_\text{samples}$. Here is the normalized accuracy score : 
$$
\texttt{accuracy}(y, \hat{y}) = \frac{1}{n_\text{samples}} \sum_{i=0}^{n_\text{samples}-1} 1(\hat{y}_i = y_i)
$$
See more details [here](https://scikit-learn.org/stable/modules/model_evaluation.html#accuracy-score) 

Here we comment the "test accuracy" that is to say the accuracy of the test sample and not on the training sample !  The test accuracy  is here **0.875**. On a **random guess it would be 0.1** : one chance to be guess the right number out of 10 possible numbers (10 classes).


```python
#Simple example of accuracy for a multiclass analysis to illustrate.
y_true = [0, 0, 1, 2, 3]
y_pred = [0, 1, 2, 1, 3]
print("accuracy normalisée : ", accuracy_score(y_true, y_pred))
print("accuracy non normalisée : ", accuracy_score(y_true, y_pred, normalize=False))
```

    accuracy normalisée :  0.4
    accuracy non normalisée :  2
    

Indeed, 

$$
\texttt{accuracy}(y, \hat{y}) = \frac{1}{n_\text{samples}} \sum_{i=0}^{n_\text{samples}-1} 1(\hat{y}_i = y_i) = \frac{1}{5} \sum_{i=0}^{4} 1(\hat{y}_i = y_i) =  \frac{1}{5} * 2 = 0.4
$$


**Question:** What is ``` LinearSVC()``` classifier? Which kernel are we using? What is ```C```? (this is a tricky question, try to find the answer online)

**Answer :** ``` LinearSVC()``` (Linear Support Vector Classification) is a fast implementation of Support Vector Machine Classification (SVM) for the case of a linear kernel. It is similar to `SVC` with parameter `kernel=’linear’`, but implemented in terms of `liblinear` rather than `libsvm`, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples.
The main characteristics of this method are : 
- the loss used is the [‘squared_hinge’](https://en.wikipedia.org/wiki/Hinge_loss) (even if it is not indicated in the [general documentation](https://scikit-learn.org/stable/modules/svm.html#linearsvc) which is strange)
- to generate the multiclass problem, ` LinearSVC()` uses `  One-vs-All` (see example [here](http://eric.univ-lyon2.fr/~ricco/cours/slides/svm.pdf), slide 38).

More precisely, given training vectors $x_i \in \mathbb{R}^p$, i=1,…, n, in two classes, and a vector $y \in \{1, -1\}^n$ 
, our goal is to find $w \in \mathbb{R}^p$ and $b \in \mathbb{R}$ such that the prediction given by $\text{sign} (w^T\phi(x) + b)$ is correct for most samples.
LinearSVC solves the following problem:
$\min_ {w, b} \frac{1}{2} w^T w + C \sum_{i=1}\max(0, y_i (w^T \phi(x_i) + b))$,
where we make use of the hinge loss. This is the form that is directly optimized by LinearSVC, but unlike the dual form, this one does not involve inner products between samples, so the famous kernel trick cannot be applied. This is why only the linear kernel is supported by LinearSVC ($\phi$ is the identity function).

The **C parameter** is a regularization or penalty parameter. SVM only work properly if the data is separable. Otherwise, we will penalize the loss of this non-separability (see [here](https://scikit-learn.org/stable/modules/svm.html#svc)) measuring the distance between the misclassified points and the separating hyperplane. C represents misclassification or error term. The misclassification or error term tells the SVM optimisation how much error is bearable. This is how you can control the trade-off between decision boundary and misclassification term. Concretely, when C is high, we penalize a lot for misclassification, which means that we classify lots of points correctly, also there is a chance to overfit.

**To learn more :**

Documentation : C is 1 by default and it’s a reasonable default choice. If you have a lot of noisy observations you should decrease it: decreasing C corresponds to more regularization. LinearSVC and LinearSVR are less sensitive to C when it becomes large, and prediction results stop improving after a certain threshold. Meanwhile, larger C values will take more time to train, sometimes up to 10 times longer.

**Question:** What is the outcome of ```np.logspace(-8, 8, 17, base=2)```? More generally, what is the outcome of ```np.logspace(-a, b, k, base=m)```?

**Answer :** 
```np.logspace(-8, 8, 17, base=2)``` returns 17 numbers spaced evenly on a log scale. The sequence starts at $2^{-8}$ and ends with $2^{8}$.

```np.logspace(-a, b, k, base=m)``` returns k numbers spaced evenly on a log scale (endpoint=True by default). The parameter `base` is the logarithmic base. In linear space, the sequence starts at $m^{-a}$ and ends at $m^b$. 

It is equivalent to
1. divide the interval $[-a,b]$ into $(y_i)_{i=1..k}$ $k$ equidistant points
2. return $\left(m^{y_i}\right)_{i=1..k}$


```python
# SVM Classifier
svc = LinearSVC(max_iter=5000)
parameters2 = {'C': np.logspace(-8, 8, 17, base=2)} # defining parameter space
clf2 = GridSearchCV(svc, parameters2, cv=3)
clf2.fit(X_train, y_train)
```

    C:\Users\Kim Antunez\Anaconda3\lib\site-packages\sklearn\svm\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
      "the number of iterations.", ConvergenceWarning)
    C:\Users\Kim Antunez\Anaconda3\lib\site-packages\sklearn\svm\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
      "the number of iterations.", ConvergenceWarning)
    




    GridSearchCV(cv=3, error_score=nan,
                 estimator=LinearSVC(C=1.0, class_weight=None, dual=True,
                                     fit_intercept=True, intercept_scaling=1,
                                     loss='squared_hinge', max_iter=5000,
                                     multi_class='ovr', penalty='l2',
                                     random_state=None, tol=0.0001, verbose=0),
                 iid='deprecated', n_jobs=None,
                 param_grid={'C': array([3.90625e-03, 7.81250e-03, 1.56250e-02, 3.12500e-02, 6.25000e-02,
           1.25000e-01, 2.50000e-01, 5.00000e-01, 1.00000e+00, 2.00000e+00,
           4.00000e+00, 8.00000e+00, 1.60000e+01, 3.20000e+01, 6.40000e+01,
           1.28000e+02, 2.56000e+02])},
                 pre_dispatch='2*n_jobs', refit=True, return_train_score=False,
                 scoring=None, verbose=0)




```python
print('Returned hyperparameter: {}'.format(clf2.best_params_))
print('Best classification accuracy in train is: {}'.format(clf2.best_score_))
print('Classification accuracy on test is: {}'.format(clf2.score(X_test, y_test)))
```

    Returned hyperparameter: {'C': 0.00390625}
    Best classification accuracy in train is: 0.8095074084579332
    Classification accuracy on test is: 0.795
    

**Question** What is the meaning of the warnings? What is the parameter responsible for its appearence?

**Answer :** Warnings are about the fact that the algorithm does not converge considering the maximum number of iterations given. The maximum number of iterations is given by the parameter `max_iter` which is here set to `max_iter=5000`.
In fact, there is no preprocessing (date are not normalize/standardized data). Therefore unscaled data can slow down or even prevent the convergence of many metric-based and gradient-based estimators. Indeed many estimators are designed with the assumption that each feature takes values close to zero or more importantly that all features vary on comparable scales.


```python
# SVM Classifier + Pipeline
pipe = Pipeline([('scaler', MaxAbsScaler()), ('svc', svc)])
parameters3 = {'svc__C': np.logspace(-8, 8, 17, base=2)} # defining parameter space
clf3 = GridSearchCV(pipe, parameters3, cv=3)
clf3.fit(X_train, y_train)
```




    GridSearchCV(cv=3, error_score=nan,
                 estimator=Pipeline(memory=None,
                                    steps=[('scaler', MaxAbsScaler(copy=True)),
                                           ('svc',
                                            LinearSVC(C=1.0, class_weight=None,
                                                      dual=True, fit_intercept=True,
                                                      intercept_scaling=1,
                                                      loss='squared_hinge',
                                                      max_iter=5000,
                                                      multi_class='ovr',
                                                      penalty='l2',
                                                      random_state=None, tol=0.0001,
                                                      verbose=0))],
                                    verbose=False),
                 iid='deprecated', n_jobs=None,
                 param_grid={'svc__C': array([3.90625e-03, 7.81250e-03, 1.56250e-02, 3.12500e-02, 6.25000e-02,
           1.25000e-01, 2.50000e-01, 5.00000e-01, 1.00000e+00, 2.00000e+00,
           4.00000e+00, 8.00000e+00, 1.60000e+01, 3.20000e+01, 6.40000e+01,
           1.28000e+02, 2.56000e+02])},
                 pre_dispatch='2*n_jobs', refit=True, return_train_score=False,
                 scoring=None, verbose=0)




```python
print('Returned hyperparameter: {}'.format(clf3.best_params_))
print('Best classification accuracy in train is: {}'.format(clf3.best_score_))
print('Classification accuracy on test is: {}'.format(clf3.score(X_test, y_test)))
```

    Returned hyperparameter: {'svc__C': 0.015625}
    Best classification accuracy in train is: 0.863002432717575
    Classification accuracy on test is: 0.84
    

**Question:** What did we change with respect to the previous run of ```LinearSVC()```?

**Answer:** A pipeline allows us to perform several operations in a row. First, we renormalize the features with `MaxAbsScaler` (using the training data), in order to put them on the same scale.
This estimator scales and translates each feature individually such that the maximal absolute value of each feature in the training set will be 1. 

For each i-th component of each vector $(X_j)$, we probably divide by the highest value (in absolute value) that is to say $X'_{i,j}=\frac{X_{i,j}}{X_{i_{max},j}}$ with $i_{max}= \underset{j}{\max}|X_{i,j}|$ .

Second, we apply the same algorithm as before (svc, a LinearSVC) to fit the training data in a 3-fold CV validation (as before) to choose the best value of the C parameter which seems to be `C = 0.015625`. 

**Question:** Explain what happens if we execute
```python
    pipe.fit(X_train, y_train)
    pipe.predict(X_test, y_test)
```
**Answer:**
`pipe.fit` works. It fits the dataset as before but not using a cross-validation but using the default `C` parameter (that is to say ...) and `max_iter=5000`. 


`pipe.predict` returns the following error : `TypeError: predict() takes 2 positional arguments but 3 were given`

The function does not work here because when we do a prevision, we do not need to enter the `Y` values, we just need the `X` ones. 
This is why the following lines work (see below). 
```python
    pipe.predict(X_test) #working
    pipe.score(X_test, y_test)  #working
```


```python
pipe.fit(X_train, y_train)
```




    Pipeline(memory=None,
             steps=[('scaler', MaxAbsScaler(copy=True)),
                    ('svc',
                     LinearSVC(C=1.0, class_weight=None, dual=True,
                               fit_intercept=True, intercept_scaling=1,
                               loss='squared_hinge', max_iter=5000,
                               multi_class='ovr', penalty='l2', random_state=None,
                               tol=0.0001, verbose=0))],
             verbose=False)




```python
pipe.predict(X_test, y_test) #not working
```


    ---------------------------------------------------------------------------

    TypeError                                 Traceback (most recent call last)

    <ipython-input-20-ed47c41d3d19> in <module>
    ----> 1 pipe.predict(X_test, y_test) #not working
    

    ~\Anaconda3\lib\site-packages\sklearn\utils\metaestimators.py in <lambda>(*args, **kwargs)
        114 
        115         # lambda, but not partial, allows help() to work with update_wrapper
    --> 116         out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)
        117         # update the docstring of the returned function
        118         update_wrapper(out, self.fn)
    

    TypeError: predict() takes 2 positional arguments but 3 were given



```python
pipe.predict(X_test) #working
```




    array(['4', '1', '6', '5', '3', '4', '1', '3', '3', '1', '0', '6', '3',
           '4', '9', '7', '6', '4', '1', '6', '1', '4', '3', '8', '9', '4',
           '7', '8', '1', '1', '5', '6', '1', '4', '0', '2', '0', '9', '9',
           '6', '2', '4', '6', '4', '9', '8', '7', '7', '0', '9', '4', '6',
           '9', '7', '5', '2', '2', '7', '1', '6', '5', '4', '2', '8', '9',
           '6', '3', '2', '8', '1', '7', '0', '1', '3', '2', '0', '9', '0',
           '0', '0', '1', '0', '8', '7', '9', '9', '2', '1', '8', '9', '3',
           '1', '5', '1', '3', '1', '3', '0', '8', '7', '0', '6', '5', '9',
           '4', '0', '2', '5', '6', '9', '7', '5', '6', '3', '9', '7', '9',
           '0', '9', '3', '9', '1', '3', '1', '3', '6', '1', '3', '8', '8',
           '2', '9', '9', '6', '2', '7', '4', '3', '9', '2', '7', '0', '8',
           '1', '2', '3', '6', '0', '8', '1', '5', '0', '0', '3', '0', '4',
           '3', '1', '3', '9', '0', '4', '3', '9', '4', '8', '4', '7', '3',
           '0', '9', '5', '8', '4', '6', '6', '3', '0', '4', '7', '0', '3',
           '1', '8', '7', '8', '0', '4', '9', '6', '7', '1', '1', '2', '2',
           '3', '6', '6', '2', '0'], dtype=object)




```python
pipe.score(X_test, y_test)  #working
```




    0.835




```python
# Logistic regression
pipe = Pipeline([('scaler', StandardScaler()), ('logreg', LogisticRegression(max_iter=5000))])
parameters4 = {'logreg__C': np.logspace(-8, 8, 17, base=2)} # defining parameter space
clf4 = GridSearchCV(pipe, parameters4, cv=3)
clf4.fit(X_train, y_train)
```




    GridSearchCV(cv=3,
                 estimator=Pipeline(steps=[('scaler', StandardScaler()),
                                           ('logreg',
                                            LogisticRegression(max_iter=5000))]),
                 param_grid={'logreg__C': array([3.90625e-03, 7.81250e-03, 1.56250e-02, 3.12500e-02, 6.25000e-02,
           1.25000e-01, 2.50000e-01, 5.00000e-01, 1.00000e+00, 2.00000e+00,
           4.00000e+00, 8.00000e+00, 1.60000e+01, 3.20000e+01, 6.40000e+01,
           1.28000e+02, 2.56000e+02])})




```python
print('Returned hyperparameter: {}'.format(clf4.best_params_))
print('Best classification accuracy in train is: {}'.format(clf4.best_score_))
print('Classification accuracy on test is: {}'.format(clf4.score(X_test, y_test)))
```

    Returned hyperparameter: {'logreg__C': 0.0078125}
    Best classification accuracy in train is: 0.8705039372205788
    Classification accuracy on test is: 0.84
    

**Question:** what is the difference between ```StandardScaler()``` and ```MaxAbsScaler()```? What are other scaling options available in ```sklearn```?

**Answer:**

- ```StandardScaler()```  : Standardize features by removing the mean and scaling to unit variance
The standard score of a sample x is calculated as:
$z = (x - u) / s$
where u is the mean of the training samples or zero if with_mean=False, and s is the standard deviation of the training samples or one if with_std=False.

Centering and scaling happen independently on each feature by computing the relevant statistics on the samples in the training set. Mean and standard deviation are then stored to be used on later data using transform.
However, the outliers have an influence when computing the empirical mean and standard deviation which shrink the range of the feature values. StandardScaler therefore cannot guarantee balanced feature scales in the presence of outliers.

- ```MaxAbsScaler()``` : see previous question for the definition. 




The **differences** between these two methods are the following : 
* ```MaxAbsScaler()``` method does not shift/center the data, and thus does not destroy any sparsity, and thus can be applied to sparse CSR or CSC matrices, unlike ```StandardScaler()``` 
* ```MaxAbsScaler()``` rescales the data et such that the absolute values are mapped in the range $[0, 1]$, unlike ```StandardScaler()```
* On positive only data, ```MaxAbsScaler()``` behaves similarly to ``MinMaxScaler``` and therefore also suffers from the presence of large outliers. 


Other scaling options available in ```sklearn```:
1. ```MinMaxScaler()``` : rescales the data set such that all feature values are in the range $[0, 1]$ As StandardScaler, MinMaxScaler is very sensitive to the presence of outliers.
```python
X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))
X_scaled = X_std * (max - min) + min
```
2. ```RobustScaler()``` : Unlike the previous scalers, the centering and scaling statistics of this scaler are based on percentiles and are therefore not influenced by a few number of very large marginal outliers (robust to outliers).
3. ```Normalizer()``` :  The norm of each feature must be equal to 1. We can use many norms : $L^1$, $L^2$, $L^\infty$ ...

The whole list of preprocessing methods is available [here](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing)


**Question:** using the previous code as an example achieve test accuracy $\geq 0.9$. You can use any method from sklearn package. Give a mathematical description of the selected method. Explain the range of considered hyperparamers.


**Answer:** We give here the examples of two methods but there are plenty of them. 

1. Example 1 : SVC Classifier (other SVM classifier but not linear)


Given training vectors $x_i \in \mathbb{R}^p$, i=1,…, n, in two classes, and a vector $y \in \{1, -1\}^n$ 
, our goal is to find $w \in \mathbb{R}^p$ and $b \in \mathbb{R}$ such that the prediction given by $\text{sign} (w^T\phi(x) + b)$ is correct for most samples.
SVC solves the following problem:
 \begin{align}\begin{aligned}\min_ {w, b, \zeta} \frac{1}{2} w^T w + C \sum_{i=1}^{n} \zeta_i\\\begin{split}\textrm {subject to } & y_i (w^T \phi (x_i) + b) \geq 1 - \zeta_i,\\
& \zeta_i \geq 0, i=1, ..., n\end{split}\end{aligned}\end{align} 

Intuitively, we’re trying to maximize the margin (by minimizing $||w||^2 = w^Tw$), while incurring a penalty when a sample is misclassified or within the margin boundary. Ideally, $the value y_i
(w^T \phi (x_i) + b)$ would be $\geq 1$ for all samples, which indicates a perfect prediction. But problems are usually not always perfectly separable with a hyperplane, so we allow some samples to be at a distance $\zeta_i$ from their correct margin boundary. The penalty term $C$ controls the strengh of this penalty (as seen above). 

The dual problem to the primal is

 \begin{align}\begin{aligned}\min_{\alpha} \frac{1}{2} \alpha^T Q \alpha - e^T \alpha\\\begin{split}
\textrm {subject to } & y^T \alpha = 0\\
& 0 \leq \alpha_i \leq C, i=1, ..., n\end{split}\end{aligned}\end{align} 

where $e$ is the vector of all ones, and $Q$ is an $n$ by $n$ positive semidefinite matrix, $Q_{ij} \equiv y_i y_j K(x_i, x_j)$, where $K(x_i, x_j) = \phi (x_i)^T \phi (x_j)$ is the kernel. The terms $\alpha_i$ are called the dual coefficients, and they are upper-bounded by $C$. This dual representation highlights the fact that training vectors are implicitly mapped into a higher (maybe infinite) dimensional space by the function $\phi$ (kernel trick).
    
2. Example 2 : Random forest

The RandomForest algorithm  is a perturb-and-combine technique specifically designed for trees. This means a diverse set of classifiers is created by introducing randomness in the classifier construction. The prediction of the ensemble is given as the averaged prediction of the individual classifiers.

As other classifiers, forest classifiers have to be fitted with two arrays: a sparse or dense array X of size [n_samples, n_features] holding the training samples, and an array Y of size [n_samples] holding the target values (class labels) for the training samples. Like decision trees, forests of trees also extend to multi-output problems (if Y is an array of size [n_samples, n_outputs]).

In random forests, each tree in the ensemble is built from a sample drawn with replacement (i.e., a bootstrap sample) from the training set. Furthermore, when splitting each node during the construction of a tree, the best split is found either from all input features or a random subset of size `max_features`. The purpose of these two sources of randomness is to decrease the variance of the forest estimator. Indeed, individual decision trees typically exhibit high variance and tend to overfit. The injected randomness in forests yield decision trees with somewhat decoupled prediction errors. By taking an average of those predictions, some errors can cancel out. Random forests achieve a reduced variance by combining diverse trees, sometimes at the cost of a slight increase in bias. In practice the variance reduction is often significant hence yielding an overall better model.

In contrast to the original publication, the scikit-learn implementation combines classifiers by averaging their probabilistic prediction, instead of letting each classifier vote for a single class.

We can vary the range of several parameters. Here we chose to move `n_estimators`. Below the different parameters possible : 

    * n_estimators = number of trees in the foreset
    * max_features = max number of features considered for splitting a node
    * max_depth = max number of levels in each decision tree
    * min_samples_split = min number of data points placed in a node before the node is split
    * min_samples_leaf = min number of data points allowed in a leaf node
    * bootstrap = method for sampling data points (with or without replacement)



The following code corresponds to the **Annex 1** in the report.


```python
# Example 1 : SVC Classifier (other SVM classifier but not linear)
from sklearn.svm import SVC
svc2 = SVC(max_iter=5000) # by default : rbf kernel

pipe = Pipeline([('scaler', MaxAbsScaler()), ('svc', svc2)])
parameters5 = {'svc__C': np.logspace(-8, 8, 17, base=2)} # defining parameter space
clf5 = GridSearchCV(pipe, parameters5, cv=3)
clf5.fit(X_train, y_train)

print('Returned hyperparameter: {}'.format(clf5.best_params_))
print('Best classification accuracy in train is: {}'.format(clf5.best_score_))
print('Classification accuracy on test is: {}'.format(clf5.score(X_test, y_test)))
```

    Returned hyperparameter: {'svc__C': 8.0}
    Best classification accuracy in train is: 0.9190022106064085
    Classification accuracy on test is: 0.945
    


```python
#Example 2 : Random forest

from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import RandomizedSearchCV # Number of trees in random forest

from pprint import pprint# Look at parameters used by our current forest
rf = RandomForestRegressor(random_state = 42)
print('Parameters currently in use:\n')
pprint(rf.get_params())
```

    Parameters currently in use:
    
    {'bootstrap': True,
     'ccp_alpha': 0.0,
     'criterion': 'mse',
     'max_depth': None,
     'max_features': 'auto',
     'max_leaf_nodes': None,
     'max_samples': None,
     'min_impurity_decrease': 0.0,
     'min_impurity_split': None,
     'min_samples_leaf': 1,
     'min_samples_split': 2,
     'min_weight_fraction_leaf': 0.0,
     'n_estimators': 100,
     'n_jobs': None,
     'oob_score': False,
     'random_state': 42,
     'verbose': 0,
     'warm_start': False}
    


```python
rf = RandomForestClassifier(max_depth=10, random_state=0) # defining classifier
n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]
parameters = {'n_estimators': n_estimators}
clf6 = GridSearchCV(rf, parameters, cv=3) #cross-validation : method 3-fold.
clf6.fit(X_train, y_train)

print('Returned hyperparameter: {}'.format(clf6.best_params_))
print('Best classification accuracy in train is: {}'.format(clf6.best_score_))
print('Classification accuracy on test is: {}'.format(clf6.score(X_test, y_test)))
```

    Returned hyperparameter: {'n_estimators': 1200}
    Best classification accuracy in train is: 0.9115044579812196
    Classification accuracy on test is: 0.93
    

## Visualizing errors

Some ```sklearn``` methods are able to output probabilities ```predict_proba(X_test)```.

**Question** There is a mistake in the following chunk of code. Fix it.

**Answer:**
The line with a mistake is the following : 
```python
axes[1, j].bar(np.arange(10), clf4.predict_proba(image.reshape(1, -1)))  # MISTAKE !
``` 
If we execute the code line by line we find the following error 
``` only size-1 arrays can be converted to Python scalars ```

It means that the two following objects do not have the same dimensions : 

```python
print(np.arange(10))
print(clf4.predict_proba(image.reshape(1, -1)))
```
```
[0 1 2 3 4 5 6 7 8 9] # 1 dimension
[[1.19370882e-01 1.08367644e-04 7.49096695e-02 7.55611181e-01
  2.56621514e-06 4.47842619e-02 2.03011570e-04 2.06422609e-03
  2.94488853e-03 9.45386443e-07]] # 2 dimensions => must be 1
```

The line must be replaced by : 

```python
axes[1, j].bar(np.arange(10), clf4.predict_proba(image.reshape(1, -1))[0]) # CORRECTION ! 
```



```python
axes = plt.subplots(2, 4)[1] 

# More details about zip() function here https://docs.python.org/3.3/library/functions.html#zip
y_pred = clf4.predict(X_test)
j = 0 # Index which iterates over plots
for true_label, pred_label, image in list(zip(y_test, y_pred, X_test)):
    if j == 4: # We only want to look at 4 first mistakes
        break
    if true_label != pred_label:
        # Plotting predicted probabilities
        #axes[1, j].bar(np.arange(10), clf4.predict_proba(image.reshape(1, -1)))  # MISTAKE !
        axes[1, j].bar(np.arange(10), clf4.predict_proba(image.reshape(1, -1))[0]) # CORRECTION ! 
        axes[1, j].set_xticks(np.arange(10))
        axes[1, j].set_yticks([])
        
        # Plotting the image
        axes[0, j].imshow(image.reshape((28, 28)), cmap=plt.cm.gray_r, interpolation='nearest')
        axes[0, j].set_xticks([])
        axes[0, j].set_yticks([])
        axes[0, j].set_title('Predicted {}'.format(pred_label))
        j += 1
```


    <IPython.core.display.Javascript object>



<img src="data:image/png;base64,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" width="640">


## Changing the Loss function

It often happens that the accuracy is not the right way to evaluate the performance. ```sklearn``` has a large variety of other metrics both in classification and regression. See https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics

Here we want to understand how to change the cross-validation metric with minimal effort.


```python
# SVM Classifier + Pipeline + New score function

pipe = Pipeline([('scaler', MaxAbsScaler()), ('svc', svc)])
parameters4 = {'svc__C': np.logspace(-8, 8, 17, base=2)} # defining parameter space
balanced_scorer = make_scorer(balanced_accuracy_score)

clf4 = GridSearchCV(pipe, parameters3, cv=3, scoring=balanced_scorer)
clf4.fit(X_train, y_train)

print('Returned hyperparameter: {}'.format(clf4.best_params_))
print('Best Balanced accuracy in train is: {}'.format(clf4.best_score_))
print('Balanced accuracy on test is: {}'.format(clf4.score(X_test, y_test)))
```

    Returned hyperparameter: {'svc__C': 0.015625}
    Best Balanced accuracy in train is: 0.8612334093654231
    Balanced accuracy on test is: 0.825627008328415
    

**Question:** What is ```balanced_accuracy_score```? Write its mathematical mathematical description.

**Answer:**  The ```balanced_accuracy_score``` function computes the balanced accuracy, which **avoids inflated performance estimates on imbalanced datasets**(when classes are over or under-representated). It is the macro-average of recall scores per class or, equivalently, raw accuracy where each sample is weighted according to the inverse prevalence of its true class. Thus for balanced datasets, the score is equal to accuracy.

In the *binary case*, balanced accuracy is equal to the arithmetic mean of sensitivity (true positive rate) and specificity (true negative rate), or the area under the ROC curve with binary predictions rather than scores:

$$
\texttt{balanced-accuracy} = \frac{1}{2}\left( \frac{TP}{TP + FN} + \frac{TN}{TN + FP}\right )
$$

In the *multiclass case*, balanced accuracy is defined as the following,  given $y_i$ the true value of the $i$-th sample, predicted value $\hat{y}_i$ and $w_i$ is the corresponding sample weight.

$$
balanced-accuracy(y,\hat y,w)=\frac{1}{\sum \hat w_i} \sum_i 1_{(\hat y_i = y_i)}\hat w_i
\quad \text{ with } \quad \hat{w}_i = \frac{w_i}{\sum_j{1(y_j = y_i) w_j}}
$$

*Source: [here](https://scikit-learn.org/stable/modules/model_evaluation.html#balanced-accuracy-score).*


**To learn more :**

If the classifier performs equally well on either class, this term reduces to the conventional accuracy (i.e., the number of correct predictions divided by the total number of predictions). In contrast, if the conventional accuracy is above chance only because the classifier takes advantage of an imbalanced test set, then the balanced accuracy, as appropriate, will drop to $\frac{1}{n_{classes}}$

For a binary case, the score ranges from 0 to 1, or when `adjusted=True` is used, it rescaled to the range $\frac{1}{1-n_{classes}}$ to 1, inclusive, with performance at random scoring 0. For a multiclasses analysis, tith `adjusted=True`, balanced accuracy reports the relative increase from $\texttt{balanced-accuracy}(y, \mathbf{0}, w) =
\frac{1}{n\_classes}$. In the binary case, this is also known as *Youden’s J statistic*, or informedness.



```python
#Let's go back to our simple example for a multiclass analysis to illustrate
#and calculate the balanced-accuracy
y_true = [0, 0, 1, 2, 3]
y_pred = [0, 1, 2, 1, 3]
print("balanced-accuracy : ", balanced_accuracy_score(y_true, y_pred))
```

    balanced-accuracy :  0.375
    

Indeed, 

$$
balanced-accuracy(y,\hat y,w)=\frac{1}{\sum \hat w_i} \sum_i 1_{(\hat y_i = y_i)}\hat w_i = \frac{1}{0.5 + 0.5 + 1 + 1 + 1} \sum_i 1_{(\hat y_i = y_i)}\hat w_i = \frac{1}{4} (0.5 + 1) = \frac{3}{8} = 0.375
$$


Sometimes it is important to look at the confusion matrix of the prediction.

**Question:** What is the confusion matrix? What are the conclusions that we can draw from the ```confusion_matrix(y_test, clf4.predict(X_test))```

**Answer:** By definition a confusion matrix $C$ is such that  $C_{i,j}$ is equal to the number of observations known to be in group $i$ and predicted to be in group $j$.
Here, when can see for example that : 

* 0 are well identified (all predicted as being 0's)
* 3 are sometimes (3 times out of 23) identified as 5
* 8 are also sometimes (3 times out of 17) identified as 5


```python
print(confusion_matrix(y_test, clf4.predict(X_test)))
```

    [[22  0  0  0  0  0  0  0  0  0]
     [ 0 24  0  0  0  0  0  0  2  0]
     [ 0  0 14  1  1  0  0  0  0  0]
     [ 0  0  0 18  0  3  0  0  1  1]
     [ 0  1  0  0 17  0  0  0  0  2]
     [ 1  0  0  1  0  6  0  1  0  1]
     [ 1  2  1  0  0  0 20  0  0  0]
     [ 0  0  0  0  1  0  0 15  0  0]
     [ 0  2  0  1  0  3  0  0 11  0]
     [ 0  0  0  0  2  0  0  2  1 21]]
    

# PART 2 -- Problem

The data that we have contains images with $10$ classes. Normally, accuracy is a reasonable choice of the loss function to be optimized, but in this problem we *really* do not like when digits from $\{5, 6, 7, 8, 9\}$ are predicted to be from $\{0, 1, 2, 3, 4\}$.

When writing your report on this part, include:
   1. description of your loss function
   2. description of the pipeline
   3. description of the algorithms that you used 

The following code corresponds to the **Annex 2** in the report.

**Question:** Propose a loss function that would address our needs. Explain your choice.

**Answer:**

## First thought

For this problem, our **FIRST IDEA** was to define two sets :

1. Class 1 = {0,1,2,3,4} 
2. Class 0 = {5,6,7,8,9} 

In order to find a proper loss function, we use the definition of the precision score.
In fact, we want to minimize Y_pred in class 1 when Y_true in class 0 (reduce False Positive rate). 
In a binary classification task, the terms ‘’positive’’ and ‘’negative’’ refer to the classifier’s prediction, and the terms ‘’true’’ and ‘’false’’ refer to whether that prediction corresponds to the external judgment (sometimes known as the ‘’observation’’). In this context, we can define the notiont of precision as : $\text{precision} = \frac{tp}{tp + fp},$ which is the indicator which is the most interesting to answer to this problem.

 **=> After training the model, we compare our results with the previous confusion matrix to verify that the bottom right square has evolved well : the sum of the last five lines and the first five columns should be smaller.**


```python
#Define New precision score function with 2 class : Class 1 = {0,1,2,3,4} and Class 0 = {5,6,7,8,9}
def custom_precision_score(y_true, y_pred):
    #Class 1 = {0,1,2,3,4} and Class 0 = {5,6,7,8,9}
    #Calcul TP (True Positive) = y_true and y_pred in class 1
    true_positive = np.sum((y_true.astype(int) < 5 ) & (y_pred.astype(int) < 5))
    #Calcul FP (False Positive) = y_true in class 0 and y_pred in class 1
    false_positive = np.sum((y_true.astype(int) > 4) & (y_pred.astype(int) < 5))
    return true_positive / (true_positive + false_positive)
```

**Question:** Following above examples, make an ML pipeline that uses *your* loss function and finds appropriate classifiers.




```python
def use_svc(model):
    #define the scorer
    scorer = make_scorer(custom_precision_score, greater_is_better=True)
    pipe = Pipeline([('scaler', MaxAbsScaler()), ('svc', model)])
    parameters = {'svc__C': np.logspace(-8, 8, 17, base=2)} # defining parameter space
    clf = GridSearchCV(pipe, parameters, cv=3, scoring=scorer)
    clf.fit(X_train, y_train)
    return clf
```


```python
#Tests on different models
def evaluation_model(model):
    grid = use_svc(model)
    print('Returned hyperparameter: {}'.format(grid.best_params_))
    print('Best accuracy in train is: {}'.format(grid.best_score_))
    print('Accuracy on test is: {}'.format(grid.score(X_test, y_test)))
    best_model = grid.best_estimator_
    y_pred = best_model.predict(X_test)
    print('(Check custom precision score in test is the same as accuracy : {})'.format(custom_precision_score(y_test, y_pred)))
    print('Confusion matrix: \n {}'.format(confusion_matrix(y_test, y_pred)))
```


```python
from sklearn.svm import SVC

svc_linear = LinearSVC(max_iter=5000)
svc = SVC(max_iter=5000)

dict_of_models = {'Linear SVC': svc_linear,
                  'SVC (kernel=rbf)': svc
                  }
for name, model in dict_of_models.items():
    print('------ Model : {} ------ '.format(name))
    evaluation_model(model)
    print("\n")
```

    ------ Model : Linear SVC ------ 
    Returned hyperparameter: {'svc__C': 0.125}
    Best accuracy in train is: 0.9222826688026527
    Accuracy on test is: 0.9279279279279279
    (Check custom precision score in test is the same as accuracy : 0.9279279279279279)
    Confusion matrix: 
     [[22  0  0  0  0  0  0  0  0  0]
     [ 0 23  0  2  0  0  0  0  0  1]
     [ 1  0 14  1  0  0  0  0  0  0]
     [ 0  0  1 20  0  0  0  0  1  1]
     [ 0  1  1  0 17  0  0  0  1  0]
     [ 1  0  0  1  0  7  0  1  0  0]
     [ 1  0  0  0  0  1 20  0  1  1]
     [ 0  0  0  0  1  0  0 14  0  1]
     [ 0  1  0  1  0  2  1  0 12  0]
     [ 0  0  0  0  2  0  0  2  1 21]]
    
    
    ------ Model : SVC (kernel=rbf) ------ 
    Returned hyperparameter: {'svc__C': 2.0}
    Best accuracy in train is: 0.9379540095272763
    Accuracy on test is: 0.9629629629629629
    (Check custom precision score in test is the same as accuracy : 0.9629629629629629)
    Confusion matrix: 
     [[22  0  0  0  0  0  0  0  0  0]
     [ 0 24  1  1  0  0  0  0  0  0]
     [ 0  0 16  0  0  0  0  0  0  0]
     [ 0  0  0 21  0  1  0  0  0  1]
     [ 0  0  1  0 18  0  0  0  0  1]
     [ 0  0  0  0  0 10  0  0  0  0]
     [ 0  1  1  0  0  0 22  0  0  0]
     [ 0  0  0  0  0  0  0 16  0  0]
     [ 0  1  0  1  0  0  0  0 15  0]
     [ 0  0  0  0  0  0  0  1  1 24]]
    
    
    

Conclusions :
1. The sum of the left-bottom corner of the confusion-matrix of LinearSVC is smaller and thus better for the new method based on precision (see following matrix on the left) than for the method based on accuracy (matrix on the right. 


        [1 0 0 1 0]  [0 0 0 0 0]
        [1 2 1 0 0]  [0 1 1 0 0]
        [0 0 0 0 1]  [0 0 0 0 0]
        [0 2 0 1 0]  [0 1 0 1 1]
        [0 0 0 0 2]  [0 0 0 0 0]


2. The accuracy of the model obtained with the kernel rbf of SVC method seems to be better than with linear kernel.

**=> However, this method does not allow to penalize the error of classification inside the two classes (1 and 0). For example, if I predict a 0 instead of a 1 it is not penalized in the loss function whereas it should be.**




##  Second thought

Our **SECOND IDEA** was to change the loss function in another way to take into account the fact that we must penalize if we predict the wrong number even if the actual or predicted values are both small or large


In the usual classification that we made above, we used the following loss 
$$l_1(y,\hat{y}) = 1_{\hat{y} \ne y}$$

Here, we can modify a little bit to show that we do not like if $y \in H = \{5, 6, 7, 8, 9\}$ and $\hat{y} \in L = \{0, 1, 2, 3, 4\}$. 

$$
l_2(y,\hat{y}) = 1_{(\hat{y} \ne y) \& [(y \notin H) \text{ or } (\hat{y} \notin L)]} + \alpha * 1_{(\hat{y} \ne y) \& (y \in H) \& (\hat{y} \in L)}
$$

where $\alpha > 1$ reflects the aversion that you have when $y \in H$ and $\hat{y} \in L$.



```python
import pandas as pd

# this function returns a vector containing the loss for each pair of (y_true, y_pred)
def my_custom_loss_func(y_true,y_pred, alpha):
    df = pd.DataFrame({'y_true': [int(s) for s in y_true],'y_pred': [int(s) for s in y_pred]})
    df['loss'] = 1* (df['y_true'] != df['y_pred'])
    df['loss'] = df['loss'] + (alpha-1)*((df['y_true'] >4) & (df['y_pred'] < 5))
    return df['loss']
```


```python
# an example to illustrate it
y_true = ['9', '9', '9'] #we have 3 nines in the dataset
y_pred = ['9', '8', '1'] 
#the first one is well-predicted (loss 0),
#the second one is bad predicted but still high (loss 1)
#the last one is very bad predicted (with a low number) (loss alpha=2)
print(my_custom_loss_func(y_true,y_pred, alpha=2))
```

    0    0
    1    1
    2    2
    Name: loss, dtype: int32
    

Then, we need to define the scoring parameter to evaluate the predictions on the test set. All scorer objects follow the convention that higher return values are better than lower return values.

Previously we saw, two kinds of scorers : 

* The Accuracy classification score.
* The Balanced accuracy classification score. 



Here, we try to adapte theses scorers to take into account that we really don't like when $y \in H$ and $\hat{y} \in L$. We propose two new scorers : 

* The Accuracy "2" classification score. 
$$
\texttt{accuracy}_{2}(y, \hat{y}) = 1- (\frac{1}{\alpha * n_\text{samples}} \sum_{i=0}^{n_\text{samples}-1} l_2(y,\hat{y})) = 1- (\frac{1}{\alpha * n_\text{samples}} \sum_{i=0}^{n_\text{samples}-1} 1_{(\hat{y} \ne y) \& [(y \notin H) \text{ or } (\hat{y} \notin L)]} + \alpha * 1_{(\hat{y} \ne y) \& (y \in H) \& (\hat{y} \in L)}) 
$$

* The Balanced accuracy "2" classification score. 

$$
balanced-accuracy_2(y,\hat y,w)=1 - (\frac{1}{\alpha * \sum \hat w_i} \sum_i l_2(y,\hat{y}) \hat w_i)
$$

with still
$$
\hat{w}_i = \frac{w_i}{\sum_j{1(y_j = y_i) w_j}}
$$






```python
#both functions evaluate the quality of the prediction.

#accuracy_2 function
def accuracy_2(y_true,y_pred, alpha):
    nsamples = len(y_true)
    loss = my_custom_loss_func(y_true,y_pred,alpha)
    score = 1-((1/(alpha*nsamples)) * sum(loss))
    return score

#balanced_accuracy_2 function
def balanced_accuracy_2(y_true,y_pred, alpha):
    C = confusion_matrix(y_true, y_pred, sample_weight=None)    
    with np.errstate(divide='ignore', invalid='ignore'):
        wi_hat_par_cat = 1/C.sum(axis=1)
    if np.any(np.isinf(wi_hat_par_cat)):
        wi_hat_par_cat = wi_hat_par_cat[~np.isinf(wi_hat_par_cat)]    

    wi_hat= [None] * len(y_true)
    for i in range(len(y_true)):     #np.unique(y_true):
        for j in range(len(wi_hat_par_cat)): #np.unique(y_true): 
            if y_true[i]==np.unique(y_true)[j]: 
                wi_hat[i]=wi_hat_par_cat[j]
    loss = my_custom_loss_func(y_true,y_pred,alpha)
    score = 1-((1/(alpha*sum(wi_hat))) * sum(loss*wi_hat))
    return(score)

```


```python
# going back to the previous example to illustrate it
y_true = ['9', '9', '9'] 
y_pred = ['9', '8', '1'] 
print("accuracy_2 : ", accuracy_2(y_true,y_pred,alpha=2))
print("balanced_accuracy_2 : ", balanced_accuracy_2(y_true,y_pred,alpha=2)) 
```

    accuracy_2 :  0.5
    balanced_accuracy_2 :  0.5
    

**Question:** Following above examples, make an ML pipeline that uses *your* loss function and finds appropriate classifiers.

**Answer:**  Now that we have defined the loss and the score functions, let's try to use this score with the 3 different methods (KNN, LinearSVC and LogisticRegression) of machine learning we discovered for this TP and evaluate them. Note that we still want that the sum of the last five lines and the first five columns of the confusion matrix should be smaller than before.

**=> Whereas the change of the loss doesn't change the model obtained with the knn method, with the LinearSVC, we obtained different results from the previous models, with a confusion matrix evolving in the good way (smaller sum of the bottom left quarter). See details in the code.**


```python
# function which calculates the sum of the last five lines and five first columns of C
def sum_unwanted(clf):
    C=confusion_matrix(y_test, clf.predict(X_test))
    res = sum(C[5][:4])+sum(C[6][:5])+sum(C[7][:6])+sum(C[8][:7])+sum(C[9][:8])
    return(res)
```


```python
# function which evaluates the model
def evaluation_model(clf):
    print('Returned hyperparameter: {}'.format(clf.best_params_))
    print('Best classification accuracy2 in train is: {}'.format(clf.best_score_))
    print('Classification accuracy2 on test is: {}'.format(clf.score(X_test, y_test)))
    print('Confusion matrix: \n', confusion_matrix(y_test, clf.predict(X_test)))
```

### KNN


```python
def use_knn(alpha, balanced=False):
    if not balanced:
        scorer = make_scorer(accuracy_2, alpha=alpha)
    else:
        scorer = make_scorer(balanced_accuracy_2, alpha=alpha)   
    knn = KNeighborsClassifier() # defining classifier
    parameters = {'n_neighbors': [1, 2, 3, 4, 5]} # defining parameter space
    clf = GridSearchCV(knn, parameters, cv=3, scoring=scorer)
    clf.fit(X_train, y_train)
    return(clf)
```

Si $\alpha = 1$, we logically have the same results as the beginning of this TP: 


```python
clf7_alpha1 = use_knn(alpha=1, balanced=False)
evaluation_model(clf7_alpha1)
```

    Returned hyperparameter: {'n_neighbors': 1}
    Best classification accuracy2 in train is: 0.891497944721333
    Classification accuracy2 on test is: 0.875
    Confusion matrix: 
     [[21  0  0  0  0  0  1  0  0  0]
     [ 0 26  0  0  0  0  0  0  0  0]
     [ 0  0 14  0  0  2  0  0  0  0]
     [ 0  0  0 19  0  2  0  0  1  1]
     [ 0  1  0  0 17  0  0  0  0  2]
     [ 0  0  0  0  1  7  1  0  1  0]
     [ 0  0  0  0  0  1 23  0  0  0]
     [ 0  0  0  0  1  0  0 15  0  0]
     [ 0  1  0  1  0  0  0  0 14  1]
     [ 1  1  0  0  2  0  0  3  0 19]]
    

We try to increase the value of $\alpha$ that is to say to penalize more when $y \in H$ and $\hat{y} \in L$. let's try $\alpha=10$. 


```python
clf7_alpha10 = use_knn(alpha=10, balanced=False)
evaluation_model(clf7_alpha10)
```

    Returned hyperparameter: {'n_neighbors': 1}
    Best classification accuracy2 in train is: 0.9590000045022534
    Classification accuracy2 on test is: 0.9515
    Confusion matrix: 
     [[21  0  0  0  0  0  1  0  0  0]
     [ 0 26  0  0  0  0  0  0  0  0]
     [ 0  0 14  0  0  2  0  0  0  0]
     [ 0  0  0 19  0  2  0  0  1  1]
     [ 0  1  0  0 17  0  0  0  0  2]
     [ 0  0  0  0  1  7  1  0  1  0]
     [ 0  0  0  0  0  1 23  0  0  0]
     [ 0  0  0  0  1  0  0 15  0  0]
     [ 0  1  0  1  0  0  0  0 14  1]
     [ 1  1  0  0  2  0  0  3  0 19]]
    

The accuracy is better than the previous one but, be careful, we cannot compare the two accuracies because they are not using the same formula because it depends on the value of $\alpha$.

Unfortunately, the confusion matrix does not change because the best model remains the same (`n_neighbours = 1`) and do not decrease. However, this is totally normal because the parameter `n_neighbours = 1` already corresponds to the minimum :


```python
for n in [1, 2, 3, 4, 5]:
    knn = KNeighborsClassifier(n_neighbors = n);
    knn.fit(X_train, y_train)
    print('With parameter ',n, 'the sum of the left-bottom quarter of C is ', sum_unwanted(knn))
```

    With parameter  1 the sum of the left-bottom quarter of C is  10
    With parameter  2 the sum of the left-bottom quarter of C is  17
    With parameter  3 the sum of the left-bottom quarter of C is  15
    With parameter  4 the sum of the left-bottom quarter of C is  14
    With parameter  5 the sum of the left-bottom quarter of C is  13
    

### LinearSVC


```python
def use_svc(alpha, balanced=False,linear=True):
    #define the model
    if linear:
         model = LinearSVC(max_iter=5000)
    else:
         model =  SVC(max_iter=5000)
    #define the scorer
    if not balanced:
        scorer = make_scorer(accuracy_2, alpha=alpha)
    else:
        scorer = make_scorer(balanced_accuracy_2, alpha=alpha)  
        
    pipe = Pipeline([('scaler', MaxAbsScaler()), ('svc', model)])
    parameters = {'svc__C': np.logspace(-8, 8, 17, base=2)} # defining parameter space
    clf = GridSearchCV(pipe, parameters, cv=3, scoring=scorer)
    clf.fit(X_train, y_train)
    return(clf)
```

Example of the LinearSVC model with a balanced_accuracy function. With $\alpha = 1$ the results are obviously the same as previously that is to say : 


```python
clf8_alpha1 = use_svc(alpha=1, balanced=True,linear=True)
evaluation_model(clf8_alpha1)
```

    Returned hyperparameter: {'svc__C': 0.015625}
    Best classification accuracy2 in train is: 0.8612334093654243
    Classification accuracy2 on test is: 0.8256270083284148
    Confusion matrix: 
     [[22  0  0  0  0  0  0  0  0  0]
     [ 0 24  0  0  0  0  0  0  2  0]
     [ 0  0 14  1  1  0  0  0  0  0]
     [ 0  0  0 18  0  3  0  0  1  1]
     [ 0  1  0  0 17  0  0  0  0  2]
     [ 1  0  0  1  0  6  0  1  0  1]
     [ 1  2  1  0  0  0 20  0  0  0]
     [ 0  0  0  0  1  0  0 15  0  0]
     [ 0  2  0  1  0  3  0  0 11  0]
     [ 0  0  0  0  2  0  0  2  1 21]]
    

We can notice that, here, the parameter of the initial model `C=0.015625`is not the one which minimizes the sum of the bottom left quarter of the confusion matrix. The minimum is $13$ for `C=0.125`.


```python
for c in np.logspace(-8, 8, 17, base=2):
    model = LinearSVC(max_iter=5000,C=c)
    pipe = Pipeline([('scaler', MaxAbsScaler()), ('svc', model)])
    pipe.fit(X_train, y_train)
    print('With parameter ',c, 'the sum of the left-bottom quarter of C is ', sum_unwanted(pipe))
```

    With parameter  0.00390625 the sum of the left-bottom quarter of C is  18
    With parameter  0.0078125 the sum of the left-bottom quarter of C is  17
    With parameter  0.015625 the sum of the left-bottom quarter of C is  17
    With parameter  0.03125 the sum of the left-bottom quarter of C is  14
    With parameter  0.0625 the sum of the left-bottom quarter of C is  15
    With parameter  0.125 the sum of the left-bottom quarter of C is  13
    With parameter  0.25 the sum of the left-bottom quarter of C is  14
    With parameter  0.5 the sum of the left-bottom quarter of C is  15
    With parameter  1.0 the sum of the left-bottom quarter of C is  14
    With parameter  2.0 the sum of the left-bottom quarter of C is  16
    With parameter  4.0 the sum of the left-bottom quarter of C is  16
    With parameter  8.0 the sum of the left-bottom quarter of C is  16
    With parameter  16.0 the sum of the left-bottom quarter of C is  16
    

    C:\Users\Kim Antunez\Anaconda3\lib\site-packages\sklearn\svm\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
      "the number of iterations.", ConvergenceWarning)
    

    With parameter  32.0 the sum of the left-bottom quarter of C is  16
    

    C:\Users\Kim Antunez\Anaconda3\lib\site-packages\sklearn\svm\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
      "the number of iterations.", ConvergenceWarning)
    

    With parameter  64.0 the sum of the left-bottom quarter of C is  16
    With parameter  128.0 the sum of the left-bottom quarter of C is  16
    With parameter  256.0 the sum of the left-bottom quarter of C is  16
    

    C:\Users\Kim Antunez\Anaconda3\lib\site-packages\sklearn\svm\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
      "the number of iterations.", ConvergenceWarning)
    

With $\alpha = 1000$, we get the expected result because the best model is now the one with `C=0.125` and the confusion matrix did changed in the good way.


```python
clf8_alpha1000 = use_svc(alpha=1000, balanced=True,linear=True)
evaluation_model(clf8_alpha10)
```

    Returned hyperparameter: {'svc__C': 0.015625}
    Best classification accuracy2 in train is: 0.9484628402030914
    Classification accuracy2 on test is: 0.9211322709685881
    Confusion matrix: 
     [[22  0  0  0  0  0  0  0  0  0]
     [ 0 24  0  0  0  0  0  0  2  0]
     [ 0  0 14  1  1  0  0  0  0  0]
     [ 0  0  0 18  0  3  0  0  1  1]
     [ 0  1  0  0 17  0  0  0  0  2]
     [ 1  0  0  1  0  6  0  1  0  1]
     [ 1  2  1  0  0  0 20  0  0  0]
     [ 0  0  0  0  1  0  0 15  0  0]
     [ 0  2  0  1  0  3  0  0 11  0]
     [ 0  0  0  0  2  0  0  2  1 21]]
    

### Logistic regression


```python
def use_logistic(alpha, balanced=False):
    #define the scorer
    if not balanced:
        scorer = make_scorer(accuracy_2, alpha=alpha)
    else:
        scorer = make_scorer(balanced_accuracy_2, alpha=alpha)  
    pipe = Pipeline([('scaler', StandardScaler()), ('logreg', LogisticRegression(max_iter=5000))])
    parameters = {'logreg__C': np.logspace(-8, 8, 17, base=2)} # defining parameter space
    clf = GridSearchCV(pipe, parameters, cv=3, scoring=scorer)
    clf.fit(X_train, y_train)
    return(clf)
```

Example of the LogisticRegression model with a accuracy function. First, with $\alpha = 1$ :


```python
clf9_alpha1 = use_logistic(alpha=1, balanced=True)
evaluation_model(clf9_alpha1)
```

    Returned hyperparameter: {'logreg__C': 0.0078125}
    Best classification accuracy2 in train is: 0.8692423758419983
    Classification accuracy2 on test is: 0.8337791822414583
    Confusion matrix: 
     [[22  0  0  0  0  0  0  0  0  0]
     [ 0 21  0  3  0  0  0  0  2  0]
     [ 0  0 13  1  1  0  1  0  0  0]
     [ 0  0  1 17  0  3  0  0  1  1]
     [ 0  1  0  0 18  0  0  0  0  1]
     [ 1  0  0  0  0  8  0  1  0  0]
     [ 1  1  1  0  0  0 20  0  1  0]
     [ 0  0  0  0  1  0  0 14  0  1]
     [ 0  2  0  1  0  3  0  0 11  0]
     [ 0  0  0  0  0  0  0  2  0 24]]
    

We can notice that, here, the parameter of the initial model `0.0078125` already the one which minimizes the sum of the bottom left quarter of the confusion matrix. The minimum is also the same ($13$) for `C=0.00390625`  and `C=0.25`.


```python
for c in np.logspace(-8, 8, 17, base=2):
    pipe = Pipeline([('scaler', StandardScaler()), ('logreg', LogisticRegression(max_iter=5000,C = c))])
    pipe.fit(X_train, y_train)
    print('With parameter ',c, 'the sum of the left-bottom quarter of C is ', sum_unwanted(pipe))
```

    With parameter  0.00390625 the sum of the left-bottom quarter of C is  13
    With parameter  0.0078125 the sum of the left-bottom quarter of C is  13
    With parameter  0.015625 the sum of the left-bottom quarter of C is  16
    With parameter  0.03125 the sum of the left-bottom quarter of C is  15
    With parameter  0.0625 the sum of the left-bottom quarter of C is  14
    With parameter  0.125 the sum of the left-bottom quarter of C is  14
    With parameter  0.25 the sum of the left-bottom quarter of C is  13
    With parameter  0.5 the sum of the left-bottom quarter of C is  14
    With parameter  1.0 the sum of the left-bottom quarter of C is  14
    With parameter  2.0 the sum of the left-bottom quarter of C is  14
    With parameter  4.0 the sum of the left-bottom quarter of C is  15
    With parameter  8.0 the sum of the left-bottom quarter of C is  15
    With parameter  16.0 the sum of the left-bottom quarter of C is  15
    With parameter  32.0 the sum of the left-bottom quarter of C is  15
    With parameter  64.0 the sum of the left-bottom quarter of C is  15
    With parameter  128.0 the sum of the left-bottom quarter of C is  15
    With parameter  256.0 the sum of the left-bottom quarter of C is  15
    
